{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you'll practice creating ordinary least squares, ridge, and LASSO regression models using the [Iris Dataset](https://archive.ics.uci.edu/ml/datasets/iris). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the data and do some minor preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = pd.read_csv('iris.csv')\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop irrelevant columns\n",
    "iris_data = iris.drop(columns = ['Id', 'Species'])\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we generate the train/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_train, iris_test = train_test_split(iris_data, test_size = 0.2, random_state = 0) # split into training set and test set\n",
    "iris_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now, create a regular linear regression model to predict the sepal length of iris flowers given the sepal width, petal length, and petal width using the process we've learned in lecture (fitting, predicting, finding the error/loss)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# X_train contains our features, y_train contains the values we're trying to predict\n",
    "X_train = iris_train.drop(...)\n",
    "y_train = iris_train[...]\n",
    "\n",
    "X_test = iris_test.drop(...)\n",
    "y_test = iris_test[...]\n",
    "\n",
    "# instantiate your OLS model\n",
    "ols_model = ...\n",
    "\n",
    "# fit the model\n",
    "ols_model.fit(..., ...)\n",
    "# make predictions on test set\n",
    "y_pred = ols_model.predict(...)\n",
    "# find mean squared error\n",
    "ols_loss = mean_squared_error(..., ...)\n",
    "ols_r2 = r2_score(..., ...)\n",
    "\n",
    "print(\"Mean Squared Error of Linear Model: {:.3f}\".format(ols_loss))\n",
    "print(\"R^2 of Linear Model: {:.3f}\".format(lin_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Strategy for Hyperparameter Selection: K-Fold Cross Validation\n",
    "\n",
    "Earlier in lecture, we discussed the K-fold cross validation method for selecting hyperparameters for our model, namely the Ridge and LASSO models. We will proceed by fitting K models per choice of hyperparameter, and to emphasize what K-fold cross validation actually means, we're going to manually carry out the procedure. Recall the approach looks something like the figure below for 4-fold cross validation:\n",
    "\n",
    "<img src=\"cv.png\" width=500px>\n",
    "\n",
    "When we use K-fold cross validation, to select between various hyperparameters, we split the training set further into multiple temporary train and validation sets (each split is called a \"fold\", hence k-fold cross validation). We will use the average validation error across all k folds to make our optimal feature, model, and hyperparameter choices. In this example, we'll only use this procedure for hyperparameter selection, specifically to choose the best alpha, for both Ridge and LASSO models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Scikit-learn has built-in support for cross validation.  However, to better understand how cross validation works complete the following function which cross validates a given model.\n",
    "\n",
    "1. Use the [`KFold.split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) function to get 4 splits on the training data. Note that `split` returns the indices of the data for that split.\n",
    "2. For **each** split:\n",
    "    1. Select out the training and validation rows and columns based on the split indices and features.\n",
    "    2. Compute the MSE on the validation split.\n",
    "    3. Return the average error across all cross validation splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_CV_error(model, X_train, y_train):\n",
    "    '''\n",
    "    Split the training data into 4 subsets.\n",
    "    For each subset, \n",
    "        fit a model holding out that subset\n",
    "        compute the MSE on that subset (the validation set)\n",
    "    You should be fitting 4 models total.\n",
    "    Return the average MSE of these 4 folds.\n",
    "\n",
    "    Args:\n",
    "        model: an sklearn model with fit and predict functions \n",
    "        X_train (data_frame): Training data\n",
    "        y_train (data_frame): output data \n",
    "\n",
    "    Return:\n",
    "        the average validation MSE for the 4 splits.\n",
    "    '''\n",
    "    # Creating a KFold object that will produce 4 splits\n",
    "    kf = KFold(n_splits=4)\n",
    "    validation_errors = []\n",
    "    \n",
    "    for train_idx, valid_idx in kf.split(X_train):\n",
    "        # split the data\n",
    "        split_X_train, split_X_valid = X_train.iloc[...], X_train.iloc[...]\n",
    "        split_y_train, split_y_valid = y_train.iloc[...], y_train.iloc[...]\n",
    "\n",
    "        # Fit the model on the training split\n",
    "        model.fit(..., ...)\n",
    "        \n",
    "        # Compute the MSE on the validation split\n",
    "        error = mean_squared_error(..., model.predict(...))\n",
    "        \n",
    "        validation_errors.append(...)\n",
    "        \n",
    "    return np.mean(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Now, use the given lambda parameters to create Ridge regression models with 5 different lambda parameters and determine which model has the lowest CV error. Then, calculate the error of that model on the test set. (As a bonus, repeat the same for LASSO regression.) We've provided an outline for how you might want to structure your code for this question; feel free to use it or write your own solution however you'd like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "lambdas = [0.01, 0.1, 1, 10, 100]\n",
    "# feel free to add more lines of code here as necessary\n",
    "...\n",
    "for param in lambdas:\n",
    "    model = Ridge(alpha=..., fit_intercept = False)\n",
    "    model_err = compute_CV_error(..., ..., ...)\n",
    "    ...\n",
    "\n",
    "best_ridge_loss = mean_squared_error(..., ...)\n",
    "ridge_r2 = r2_score(..., ...)\n",
    "\n",
    "print(\"Mean Squared Error of Ridge Model: {:.3f}\".format(ridge_loss))\n",
    "print(\"R^2 of Ridge Model: {:.3f}\".format(ridge_r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Compare the performance of the Ridge models using different values of lambda with the OLS solution. What do you notice about the validation error as lambda increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Your  answer here_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this dataset and demonstration purposes, cross validation is a viable option to use for hyperparameter selection. Cross validation can also be used to select between various features or even various model architectures!\n",
    "\n",
    "Keep in mind that in situations where models can be very memory intensive and time consuming to train (like with deep learning), you'll typically prefer using other methods to select hyperparameters like a holdout set (also known as simple cross validation, where you just use a held-out split from training data to test multiple different hyperparameters).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have completed this homework! :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
