{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-266ac0d8-75e1-497a-a768-4bca3951d210",
    "deepnote_cell_type": "markdown",
    "id": "mpGKnuj00l7r"
   },
   "source": [
    "# Lecture 9: Introduction to Classification\n",
    "### 4/19/22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-1337885d-3702-454c-b25b-3f268196340c",
    "deepnote_cell_type": "markdown",
    "id": "J5Xr5NL70l7s",
    "tags": []
   },
   "source": [
    "### Table of Contents\n",
    "* [What is Machine Learning?](#ml)  \n",
    "    * [Taxonomy of Machine Learning](#taxonomy)\n",
    "\n",
    "* [Classification](#classification)\n",
    "    * [Binary Classification](#binary)\n",
    "    * [Extending Linear Regression to Classification](#extension)\n",
    "* [Logistic Regression](#logistic_regression)\n",
    "    * [The Sigmoid Function](#sigmoid)\n",
    "    * [Fitting Logistic Regression Models](#logistic_fitting)\n",
    "    * [Logistic Loss](#log_loss)\n",
    "* [Assessing Classification Models](#assessing)\n",
    "    * [Decision Boundaries](#decisions)\n",
    "    * [Classification Metrics](#metrics)\n",
    "    * [ROC Curves](#roc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-20f8f13c-ecda-45a8-89bf-8d17c79856f0",
    "deepnote_cell_type": "markdown",
    "id": "g29SgCn10l7u"
   },
   "source": [
    "### Hosted by and maintained by the [Student Association for Applied Statistics (SAAS)](https://saas.berkeley.edu).\n",
    "\n",
    "Presented by Jonathan Pan and Gilbert Feng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-fb12f49b-9224-4714-abe6-2ddb013daac8",
    "deepnote_cell_type": "markdown",
    "id": "0il2kplr0l7u",
    "tags": []
   },
   "source": [
    "<a id='ml'></a>\n",
    "## What is Machine Learning? \n",
    "\n",
    "The term machine learning was coined in 1959 by computer scientist Arthur Samuel after his work on creating a checker playing program. Samuel defined machine learning as:\n",
    "> \"A field of study that gives computers the ability to learn without being explicitly programmed\"\n",
    "Although this quote is likely misattributed, it still serves as a good baseline definition, despite being overly abstract and ambiguous. However, Samuel's work in 1959 is by no means the moment in time at which machine learning was \"invented\". The least squares method, one of the most common methods of data fitting in machine learning, was discovered over 150 years prior in 1805 by Adrien-Marie Legendre.\n",
    "\n",
    "Some of theory and methods behind machine learning have been around for decades, so why is it so popular now?\n",
    "* Abundance of data (cloud storage)\n",
    "* Abundance of computing power (advancements in GPUs)\n",
    "* Money (companies have been able to make the above two profitable)\n",
    "\n",
    "Machine Learning is an evolving field, it has been for nearly a century and will continue to evolve for the foreseeable future. Despite this, there are some commonalities in how machine learning is used today:\n",
    "![](images/ml_steps.png)\n",
    "As you can see, there is a lot more to data science and machine learning than building linear regression models. The 7 steps above are a good summary, but the machine learning process actually is not one-directional; it resembles more of a cycle, just like the scientific method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-70a5794a-396b-475d-83cf-937cbfb848e0",
    "deepnote_cell_type": "markdown",
    "id": "1tkI3qM70l7u",
    "tags": []
   },
   "source": [
    "<a id='taxonomy'></a>\n",
    "### Taxonomy of Machine Learning\n",
    "\n",
    "The background information and definitions above are still kind of ambiguous. Let's do a better job at concretely understanding the foundation of modern machine learning.\n",
    "\n",
    "If you take a class like Data 100, you will likely see an image similar to the one below. It's not a perfect taxonomy, but it captures most of the key ideas. Let's take a few minutes to understand what's going on here. \n",
    "\n",
    "![](images/ml_taxonomy.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-ae9522de-9159-45e1-87e9-e39adf17411c",
    "deepnote_cell_type": "markdown",
    "id": "diqHplpj0l7u",
    "tags": []
   },
   "source": [
    "#### Exercise: What group of machine learning algorithms does Linear Regression fall under? \n",
    "\n",
    "**Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-d01362ac-074b-4823-9d17-b60d73f4913f",
    "deepnote_cell_type": "markdown",
    "id": "z6xzBYrC0l7v",
    "tags": []
   },
   "source": [
    "#### Exercise: What is the difference between Supervised and Unsupervised Learning? Come up with one example of a problem which falls under each one. \n",
    "\n",
    "**Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-7252b95f-29ff-4a4c-ac3d-fc552d0da371",
    "deepnote_cell_type": "markdown",
    "id": "8hTxqeRB0l7v",
    "tags": []
   },
   "source": [
    "<a id='classification'></a>\n",
    "## Classification \n",
    "\n",
    "So far in your CX journey, you've learned how to solve problems involving Regression, which is the left-most branch of the ML taxonomy tree diagram. \n",
    "\n",
    "Now we will jump over to it's sibling, *classification*.\n",
    "\n",
    "Instead of trying to predict a quantitative response variable, we'll simplify our prediction to selecting between categories. For example, consider the following classification problem of predicting whether a particular image is a cat or a dog: \n",
    "![](images/cat_dog.gif)\n",
    "In a classification problem, we are given data $\\mathbf{X}$ and labels $\\vec{y}$ and we want to learn the relationship between them. In this particular example, $\\mathbf{X}$ might represent a grid of pixels containing the image of a cat, while $\\vec{y}$ would would contain a 1 or 0, depending if the image was a cat or dog."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-6074aa05-9940-4871-a2f2-2b59523d3afe",
    "deepnote_cell_type": "markdown",
    "id": "FZRzG6mN0l7v",
    "tags": []
   },
   "source": [
    "#### Exercise: If you were asked to develop a method to classify images of cats and dogs, how would you do it?\n",
    "\n",
    "**Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-d91bcfaf-9090-4d72-998b-653f84f6ab6f",
    "deepnote_cell_type": "markdown",
    "id": "NA5YCddd0l7v",
    "tags": []
   },
   "source": [
    "<a id='binary'></a>\n",
    "### Binary Classification\n",
    "\n",
    "In general, it's totally possible to have a classification model which can make predictions between many classes all at once; for example, predicting if a given image is a dog, cat, chair, plane, etc. For simplicity today, we will focus on the *binary classification* case: you are only predicting whether a data point is a particular class (**1**) or not (**0**). \n",
    "\n",
    "One way to build a binary classification model is to think about conditional probability: $\\mathbb{P}[y_i = 1 | \\vec{x}_i]$. In words, we want to think about, what is the chance this data point has a label of 1, given its features $\\vec{x}_i$? (Note: The notation $\\vec{x}_i$ represents a row vector of all the features for a single individual). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-fc73b7e0-9d9a-406a-b4d4-a6196d4a9c60",
    "deepnote_cell_type": "markdown",
    "id": "a9GtCzoQ0l7v",
    "tags": []
   },
   "source": [
    "#### Exercise: If you knew exactly what $\\mathbb{P}[y_i = 1 | \\vec{x}_i]$ was for each individual $i$, for what range of probabilities would you classify the individual into class 1? How about class 0? \n",
    "\n",
    "**Answer**: [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-aec9fd61-488e-4863-a290-34f86ccec0c9",
    "deepnote_cell_type": "markdown",
    "id": "zq6e4tsb0l7w",
    "tags": []
   },
   "source": [
    "<a id='extension'></a>\n",
    "### Extending Linear Regression to Classification\n",
    "\n",
    "Now, we have observed that if we magically knew what $\\mathbb{P}[y_i = 1 | \\vec{x}_i]$ was for each individual, we could build our binary classifier. But, the problem is that we don't know the relationship between $\\vec{y}$ and $\\mathbf{X}$ and therefore, $\\mathbb{P}[y_i = 1 | \\vec{x}_i]$ is unknown. \n",
    "\n",
    "Instead, maybe we can **estimate** $\\mathbb{P}[y_i = 1 | \\vec{x}_i]$ using linear regression! Let's use the following formula:\n",
    "\n",
    "$$\\mathbb{P}[y_i = 1 | \\vec{x}_i] \\approx \\beta_0 + \\beta_1 x_{i,1} + \\beta_2x_{i,2} + \\dots + \\beta_d x_{i,d} = \\vec{\\beta}^T\\vec{x_i}$$\n",
    "\n",
    "Like before, the $\\beta_i$ terms represent the coefficients of each feature for a particular individual. Unfortunately, there's a problem with this setup. Let's examine the image below: \n",
    "![](images/lin_prob.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-79aa25c7-e58d-475c-9fef-15480c4bb571",
    "deepnote_cell_type": "markdown",
    "id": "KA967l260l7x",
    "tags": []
   },
   "source": [
    "#### Exercise: What are the features used in the linear probability model above? What is the model trying to predict? \n",
    "\n",
    "**Answer**: [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-70e1e2f9-9261-47d9-b6eb-e87f298bd0a0",
    "deepnote_cell_type": "markdown",
    "id": "g3MVKOk-0l7x",
    "tags": []
   },
   "source": [
    "#### Exercise: Is linear regression a valid model to predict probabilities? Why or why not? \n",
    "\n",
    "**Answer**: [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-ee3b2b20-06f8-4ce3-9ec2-e904bfac7840",
    "deepnote_cell_type": "markdown",
    "id": "9BnoVHYX0l7x",
    "tags": []
   },
   "source": [
    "<a id='logistic_regression'></a>\n",
    "## Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-c0318e74-23a0-4111-9bf4-5ca8c842bec7",
    "deepnote_cell_type": "markdown",
    "id": "iKs_TBmv0l7x",
    "tags": []
   },
   "source": [
    "<a id='sigmoid'></a>\n",
    "### The Sigmoid Function\n",
    "\n",
    "To fix the issue we observed in the previous section, we will apply a transformation to our linear model called the **sigmoid function** to ensure that the output is between [0, 1]. Here's an image of what it looks like: \n",
    "![](images/sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-27b2bd66-f62c-4a20-b1b9-6b7d2cbb9358",
    "deepnote_cell_type": "markdown",
    "id": "f0__Fcbu0l7x",
    "tags": []
   },
   "source": [
    "#### Exercise: Implement the sigmoid function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00017-1f069630-e992-434d-8fb1-82e4e6021a14",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1636793181888,
    "id": "imP5m4j90l7x",
    "outputId": "261e3903-96ca-445f-da43-eff0487a6757",
    "source_hash": "4882c57a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Returns the value of the sigmoid function for an input z.\"\"\"\n",
    "    pass # TODO: Replace this line with your implementation of sigmoid \n",
    "\n",
    "coefficients = np.array([[5, 4, 2, 1]])\n",
    "features = np.array([[1, 4, 1, 2]])\n",
    "\n",
    "# Calculate the linear prediction \n",
    "prediction = _________  @  _________ # TODO: Compute a dot product between the coefficients and features \n",
    "\n",
    "# Test sigmoid (no changes needed)\n",
    "prob = sigmoid(prediction)\n",
    "print(f'Your sigmoid function returned: {prob}')\n",
    "assert (prob <= 1) and (prob >= 0), ValueError('There\\'s something wrong with your sigmoid function implementation.')\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-beb9e96e-9686-458d-aec8-e40755feb944",
    "deepnote_cell_type": "markdown",
    "id": "XwMW0u990l7y",
    "tags": []
   },
   "source": [
    "<a id='logistic_fitting'></a>\n",
    "### Fitting Logistic Regression Models \n",
    "\n",
    "As we saw in the previous section, we will use the sigmoid function to make our predictions with Logistic Regression. We take the following steps to generate our prediction: \n",
    "- First, calculate the linear prediction: \n",
    "    $$z = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\dots + \\beta_dx_d$$\n",
    "- Next, calculate the predicted probability:\n",
    "    $$p = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "So, if $p$ is less than 0.5, we classify the data point as $0$ and if $p$ is greater than 0.5, we predict 1. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-72302acc-e35e-445b-9d40-45a4b1c40b18",
    "deepnote_cell_type": "markdown",
    "id": "oL6oXxwt0l7z",
    "tags": []
   },
   "source": [
    "#### Exercise: Let's say for a particular data point, you have a value of $z = -0.5$. What would you predict for that point: 0 or 1? \n",
    "\n",
    "**Answer**: [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-e987ce42-90e2-44aa-9889-756e16c3feef",
    "deepnote_cell_type": "markdown",
    "id": "yB4cf7a-0l7z",
    "tags": []
   },
   "source": [
    "#### Logistic Regression in Scikit-learn\n",
    "\n",
    "In practice, how can we write a model?\n",
    "\n",
    "This example explores a data set of cars, and uses characteristics of the car to predict if oil was expensive (1) or inexpensive (0) when it was released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-53a6a88b-a1f3-4204-900f-be827eff28ec",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1636799223742,
    "id": "FgO0nS-e0l7z",
    "source_hash": "1e4fbf21",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00022-bc450042-2e40-43fd-a8f6-92bd44a5662a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 48,
    "execution_start": 1636799224828,
    "id": "pQR4TOEd0l7z",
    "outputId": "9088e9bc-6384-4e74-b05c-651e1597109d",
    "source_hash": "3945ec2c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mpg_cat = pd.read_csv(\"./data/mpg_category.csv\", index_col=\"name\") \n",
    "\n",
    "# Encode OilExpensive such that 0 is inexpensive and 1 is expensive\n",
    "mpg_cat[\"OilExpensive\"] = (mpg_cat.OilExpensive == \"expensive\")*1\n",
    "mpg_cat[\"Old?\"] = (mpg_cat.loc[:, \"Old?\"] == \"old\")*1\n",
    "\n",
    "\n",
    "# Train, test split\n",
    "mpg_cat_train, mpg_cat_test = train_test_split(mpg_cat, \n",
    "                                       test_size = .2, \n",
    "                                       random_state = 0) \n",
    "\n",
    "# Train, validation split\n",
    "mpg_cat_train, mpg_cat_validation = train_test_split(mpg_cat_train, \n",
    "                                             test_size = .25, \n",
    "                                             random_state = 0)\n",
    "\n",
    "# Notice that the splitting above creates a 60/20/20 split\n",
    "print(\"Unique Values of OilExpensive: \" + str(mpg_cat.OilExpensive.unique()))\n",
    "mpg_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00024-8a28c027-09a0-4f37-8b28-73086064af21",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1636800752230,
    "id": "P211qCX_0l7z",
    "source_hash": "dc3d91e3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = np.array(mpg_cat_train.drop(\"OilExpensive\", axis=1))\n",
    "y_train = mpg_cat_train.OilExpensive.values\n",
    "\n",
    "X_val = np.array(mpg_cat_validation.drop(\"OilExpensive\", axis=1))\n",
    "y_val = mpg_cat_validation.OilExpensive.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00025-e3d9097c-9308-425d-a240-7eb7aa04ad63",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 84,
    "execution_start": 1636800754330,
    "id": "FdXYXqIM0l7z",
    "source_hash": "a11dd5b2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "lr = LogisticRegression()\n",
    "lr = lr.fit(X_train, y_train)\n",
    "y_val_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00026-2a974420-844a-4f29-afcf-18686fa3d6b1",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 14,
    "execution_start": 1636800755780,
    "id": "NIeqYHHE0l7z",
    "outputId": "4c2649dd-2b71-4bf6-f0ab-e8b9cf7bc27a",
    "source_hash": "19f81e76",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The true labels of our validation set\n",
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-02af7b60-4b96-4727-8278-89d373fdfe78",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 4,
    "execution_start": 1636800756685,
    "id": "CZmGFGY60l70",
    "outputId": "8810a529-cf69-4ea3-eb28-285f8e722253",
    "source_hash": "51e5e33f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The labels we predicted for our validation set\n",
    "y_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-bd214edd-66f0-4295-adf9-b5630103b003",
    "deepnote_cell_type": "markdown",
    "id": "JFhckDmd0l70",
    "tags": []
   },
   "source": [
    "<a id='log_loss'></a>\n",
    "### Logistic Loss\n",
    "\n",
    "Now, you've learned about how Logistic Regression makes predictions and how it is fitted to datasets. Lastly, we should learn about its loss function, called *log loss*, also sometimes called *cross entropy loss*.\n",
    "\n",
    "Recall that for the case of linear regression, we used the sum of squared residuals as our loss function:\n",
    "\n",
    "$$L(\\beta) = \\sum_{i=1}^n (x_i^T \\beta - y_i)^2  = || X \\beta - y ||_2^2$$\n",
    "\n",
    "In the case of logistic regression, we will instead use the following loss function: \n",
    "\n",
    "$$L(\\beta) = -\\sum_{i=1}^n y_ilog(p_i) + (1 - y_i)log(1 - p_i)$$\n",
    "\n",
    "where $p_i$ is the predicted probability of the $i$th row. At first, this may seem kind of random compared to what we've seen before. But, this loss function turns out to have a deep connection with the view of binary classification in terms of probabilities. Here's a brief explanation:\n",
    "\n",
    "Let's say we had 10 total data points (5 from class 1 and 5 from class 0) we wanted to use to build a classifier. Imagine each of these data points has a fixed (but unknown) probability $p$ of belonging to class 1. Therefore, we can express the likelihood of observing these 10 data points as the following:\n",
    "\n",
    "$$Lik(p) = p^5(1-p)^5$$\n",
    "\n",
    "Now, we can take a log on both sides to simplify the powers. \n",
    "\n",
    "$$Log(Lik(p)) = log(p^5(1-p)^5) = log(p^5) + log((1-p)^5)) = 5 log(p) + 5 log(1 - p)$$\n",
    "\n",
    "Notice that this \"log-likelihood\" actually looks really similar to the log loss function we wrote above! Specifically, the log-loss is the negative of the log-likelihood. If you'd like to learn more about this connection between Logistic Regression and probability, [check out this video](https://www.youtube.com/watch?v=3wqXRQzJBpE&t=1s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-3551d02f-52fd-4dc6-b32f-f1086bb3899e",
    "deepnote_cell_type": "markdown",
    "id": "oztQyDPk0l70",
    "tags": []
   },
   "source": [
    "#### Exercise: For a particular data point $i$, what would the log loss be if $y_i = 1$? \n",
    "\n",
    "**Answer**: [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-ed10324f-72cd-4e18-8652-d02493a21b30",
    "deepnote_cell_type": "markdown",
    "id": "D4EgwfYs0l70",
    "tags": []
   },
   "source": [
    "<a id='assessing'></a>\n",
    "## Assessing Classification Methods\n",
    "\n",
    "Now that we know how to make a model... how do we know if our model is good? And how do we make our model the best it can be?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-0d7843f3-e97a-42fd-a9fb-dff20beffe14",
    "deepnote_cell_type": "markdown",
    "id": "PmQ8sscu0l70",
    "tags": []
   },
   "source": [
    "<a id='decisions'></a>\n",
    "### Decision Boundaries\n",
    "\n",
    "Part of our modeling is choosing our **decision boundary**: a threshold value where if our predicted probability is below our threshold we predict 0 and if our predicted probability is above we predict 1.\n",
    "\n",
    "\n",
    "<img src=\"images/decision_boundary.png\" width=\"500\">\n",
    "\n",
    "This decision boundary is actually a parameter of our model that we can tune, we'll see how to set our decision boundary based on a few metrics. The models that find decision boundaries are called **discriminative models**. Another example of a discriminative model besides logistic regression is support vector machine (SVM).\n",
    "\n",
    "The simplest of SVMs is the hard-margin SVM which essentially calculates the decision boundary by maximizing the distance or margin from the nearest sample point to the line. Without going into too much detail, this boils down to a Quadratic Program optimization problem with a unique solution as long as the data is linearly separable. There are other more complicated SVMs, such as the soft-margin SVM, which allow more flexibility (doesn't need to be linearly separable for example), but this will be out of the scope of this lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lr_svm = SVC(kernel=\"linear\")\n",
    "lr_svm = lr_svm.fit(X_train, y_train)\n",
    "y_svm_val_pred = lr_svm.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_svm_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Models\n",
    "The other kind of models used are called **generative models**. Generative models focus more on calculating the probabilities of being a certain class for all points. As you can see below, instead of having a clear decision boundary, we instead have probabilities.\n",
    "\n",
    "<img src=\"images/gda.png\" width=\"300\">\n",
    "\n",
    "An example of a generative model is Gaussian Discriminant Analysis (GDA). GDA assumes that each class can be modeled as a Gaussian distribution and finds the distributions through MLE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train using GDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "lr_gda = QuadraticDiscriminantAnalysis()\n",
    "lr_gda = lr_gda.fit(X_train, y_train)\n",
    "y_gda_val_pred = lr_gda.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_gda_val_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-5922ad3c-c5a4-4bc1-a045-bae82fb38c43",
    "deepnote_cell_type": "markdown",
    "id": "EyawG4NS0l70",
    "tags": []
   },
   "source": [
    "<a id='metrics'></a>\n",
    "### Classification Metrics\n",
    "\n",
    "Once we have a classifier, we want to be able to evaluate it. *Is our classifier even good?* There are a variety of metrics we can use, a few of which we'll highlight in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-b06c7aa3-0c21-457e-850c-a97b740d83db",
    "deepnote_cell_type": "markdown",
    "id": "BoAsV7wF0l70",
    "tags": []
   },
   "source": [
    "#### Accuracy \n",
    "\n",
    "One metric we can look at is accuracy. Is accuracy all we need to look at? We'll soon see the answer is no!\n",
    "\n",
    "Accuracy can be defined as:\n",
    "\n",
    "$$ accuracy = \\frac{\\text{# of points classified correctly}}{\\text{# points total}} $$\n",
    "\n",
    "Simply put, accuracy is **how many predictions we got right out of our total predictions**. This is pretty useful when the classes we're looking at have roughly similar frequencies.\n",
    "\n",
    "But what if our frequencies are unbalanced? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-03069866-9da3-4cc5-b772-f40109c84c39",
    "deepnote_cell_type": "markdown",
    "id": "6nGSOnLi0l71",
    "tags": []
   },
   "source": [
    "**Side note:** When using Scikit-Learn for logistic regression, you can calculate accuracy just by calling `model.score()`\n",
    "\n",
    "Learn how!  [Scikit-Learn Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-4e72e639-e260-4ba6-8801-5c729b6a30bb",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1636800812582,
    "id": "aAkoR89P0l71",
    "outputId": "12eaaed9-0064-4b7b-9038-badb68f7d1bb",
    "source_hash": "48dc947f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is the accuracy of our model from above!\n",
    "lr.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for gda\n",
    "lr_gda.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for svm\n",
    "lr_svm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-c8533b60-e4dd-426e-b138-53708df969a8",
    "deepnote_cell_type": "markdown",
    "id": "VeQ52xTi0l71",
    "tags": []
   },
   "source": [
    "#### Exercise: Let's see when accuracy fails us...\n",
    "\n",
    "Your group is tasked with building a classifier to predict if someone is **COVID Negative (1)** or **COVID Positive (0)**. A rival group decides this is too tough of a problem, and makes their classifier assign **everyone** a negative result. \n",
    "\n",
    "Your sample is a group of 100 people. From a much more accurate test, we know **98** are truly **COVID negative**, and only **2** in our group are **COVID positive**.\n",
    "\n",
    "How accurate is this other group's classifer? Does this mean they have a good solution?\n",
    "\n",
    "**Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-158a0c0f-ed4d-4083-99d1-454c3f5d9fb8",
    "deepnote_cell_type": "markdown",
    "id": "tIoum93S0l72",
    "tags": []
   },
   "source": [
    "The above is an example of when one class (in this case, being COVID positive) is much more rare than the other class (COVID negative). Now that we've established we can't use accuracy to make every evaluation, what else can we use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-ea553af7-32ff-4e7d-8bb0-31752ec04978",
    "deepnote_cell_type": "markdown",
    "id": "CIjjUjcU0l72",
    "tags": []
   },
   "source": [
    "#### Precision / Recall\n",
    "\n",
    "For these two metrics, we should first talk about the two types of errors (and two types of successes) in classification.\n",
    "\n",
    "##### Types of classification errors\n",
    "\n",
    "True Negative (TN): We predicted negative (0), and we were right!\n",
    "\n",
    "False Negative (FN): We predicted negative (0), and we were wrong. )):\n",
    "\n",
    "False Positive (FP): We predicted positive (1), and we were wrong. \n",
    "\n",
    "True Positive (TP): We predicted positive (1), and we were right!\n",
    "\n",
    "<img src=\"images/confusion_matrix_2.png\" width=\"450\">\n",
    "\n",
    "##### Precision\n",
    "\n",
    "\n",
    "$$ precision = \\frac{\\text{TP}}{\\text{TP + FP}} $$\n",
    "\n",
    "This punishes false positives. The more false positives the lower (worse) our precision score will be.\n",
    "\n",
    "##### Recall\n",
    "\n",
    "\n",
    "$$ recall = \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "\n",
    "This punishes false negatives. The more false negatives the lower our recall score will be.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-9dcd9c5d-58cb-43a6-b46e-7b5f89bd6677",
    "deepnote_cell_type": "markdown",
    "id": "CGcxPUho0l72",
    "tags": []
   },
   "source": [
    "<a id='roc'></a>\n",
    "### ROC Curves\n",
    "\n",
    "Now, we've covered a lot of metrics to evaluate your classifier. We also established that you should consider a few metrics when evaulating, not just one. How should we consider our metrics against/alongside each other? \n",
    "\n",
    "One visualization we can consider is an **ROC curve**.\n",
    "\n",
    "This graphs the false positive rate (FPR) against the true positive rate (TPR) of our model with different thresholds. \n",
    "\n",
    "$$ \\text{false positive rate} = \\frac{\\text{FP}}{\\text{FP + TN}} $$\n",
    "\n",
    "$$ \\text{true positive rate} = \\frac{\\text{TP}}{\\text{TP + FN}} $$\n",
    "\n",
    "![](images/roc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00039-34d7b77b-f647-4e14-9331-6831b173a6f7",
    "deepnote_cell_type": "markdown",
    "id": "Xadb5SvN0l73",
    "tags": []
   },
   "source": [
    "#### Excercise: Let's think about our ROC graph...\n",
    "\n",
    "In the ROC graph above, the beginning of the line represents our model having a false positive rate of 0 and a true positive rate of 0. In this case, our threshold is so high that our model predicts negative only—so we don't have any positive predictions (hence why our true positive *and* false positive rates are both 0).\n",
    "\n",
    "Similarly, how would you explain the right end of the ROC graph above?\n",
    "\n",
    "**Answer:** [Type your answer here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-e657d133-76e3-4249-a189-eb749c3d2871",
    "deepnote_cell_type": "markdown",
    "id": "31EYaux90l73",
    "tags": []
   },
   "source": [
    "What are we looking for in our ROC curve? A perfect model would have a TPR of 1 and an FPR of 0. This corresponds to the top left of our graph, so we'd like our curve to be as close to the top left as possible (see the orange below). \n",
    "\n",
    "One indicator from ROC curves we can calculate is the **area under curve (AUC)** of our model. An AUC of 1 would represent a perfect model. Random guessing would have an AUC of 0.5 (this wouldn't be very good for our model).\n",
    "\n",
    "![](images/roc_ideal.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lecture_9.ipynb",
   "provenance": []
  },
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "5d4ad01f-f0be-4e97-a568-b667742a6344",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
