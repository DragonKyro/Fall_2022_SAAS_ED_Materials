{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import neighbors, datasets\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn import neighbors, datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import neighbors, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10: Decision Trees, Random Forest, Ensemble Learning, K-NN, Hyperparameter Tuning\n",
    "## 4/26/2022\n",
    "\n",
    "### Hosted by and maintained by [Student Association for Applied Statistics (SAAS)](https://saas.berkeley.edu/). Updated by [Jonathan Pan](mailto:jonathanpan4@berkeley.edu), [Michael Wang](mailto:wangjmichael18@berkeley.edu)\n",
    "\n",
    "#### Originally authored by [Calvin Chen](mailto:chencalvin99@berkeley.edu), [Michelle Hao](mailto:mhao@berkeley.edu), and [Patrick Chao](mailto:prc@berkeley.edu).\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees rely a series of yes or no questions to make a decision on which class an input point falls under. You've seen decision trees your entire life. Here are some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pictures/mansplaining.png' width=100%>\n",
    "<img src='pictures/meme.png' height=5px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we answer a yes or no question at every step, and depending on our answer, we either went one way or another through the tree. They are essentially the same idea as flowcharts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply this to the data science setting for a classification task. In particular, you're given a data point $X = \\begin{bmatrix} X_1 & X_2 & ... & X_k \\end{bmatrix}$, and you want to assign it a class $c$. We've seen examples of this before: logistic regression tries to assign a class $c \\in \\{0, 1\\}$ for each data point by predicting $\\mathbb{P}(X = 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a decision tree to work for this, we want to look at $X$, ask yes-no questions about its features, and assign it to a class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='dataset'></a>\n",
    "### The Dataset\n",
    "\n",
    "<img src='pictures/iris.jpg' width=\"250\" height=\"250\">\n",
    "<center> Image from: A Complete Guide to K-Nearest-Neighbors by Zakka </center>\n",
    "\n",
    "The dataset we'll be using is the [Iris Flower Dataset](https://archive.ics.uci.edu/ml/datasets/Iris). It contains a series of observations on three species of Iris (Iris setosa, Iris versicolor, and Iris virginica). Each observation contains four features: the *petal length, petal width, sepal length, and sepal width*. The **question** we're asking today is: can we predict the species of Iris from its *petal length, petal width, sepal length, and sepal width*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing the data\n",
    "iris = datasets.load_iris()\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= ['Sepal Length', 'Sepal Width','Petal Length','Petal Width'] + ['species'])\n",
    "\n",
    "#y contains the correct classifications (0, 1, 2 for each type of Iris)\n",
    "#0 = Iris Setosa,1 = Iris Versicolour,2 = Iris Virginica\n",
    "Y = iris[\"species\"]\n",
    "Y[50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal Length</th>\n",
       "      <th>Sepal Width</th>\n",
       "      <th>Petal Length</th>\n",
       "      <th>Petal Width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "      <td>0.819232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sepal Length  Sepal Width  Petal Length  Petal Width     species\n",
       "count    150.000000   150.000000    150.000000   150.000000  150.000000\n",
       "mean       5.843333     3.057333      3.758000     1.199333    1.000000\n",
       "std        0.828066     0.435866      1.765298     0.762238    0.819232\n",
       "min        4.300000     2.000000      1.000000     0.100000    0.000000\n",
       "25%        5.100000     2.800000      1.600000     0.300000    0.000000\n",
       "50%        5.800000     3.000000      4.350000     1.300000    1.000000\n",
       "75%        6.400000     3.300000      5.100000     1.800000    2.000000\n",
       "max        7.900000     4.400000      6.900000     2.500000    2.000000"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "species\n",
       "0.0    50\n",
       "1.0    50\n",
       "2.0    50\n",
       "dtype: int64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Letâ€™s now take a look at the number of instances (rows) that \n",
    "# belong to each class. We can view this as an absolute count.\n",
    "iris.groupby('species').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the Dataframe into Feature and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Sepal Length', 'Sepal Width','Petal Length','Petal Width']\n",
    "X = iris[feature_columns].values\n",
    "Y = iris['species'].values\n",
    "\n",
    "# Alternative way of selecting features and labels arrays:\n",
    "# X = dataset.iloc[:, 1:5].values\n",
    "# y = dataset.iloc[:, 5].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting the Data into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting dataset into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_iris, X_test_iris, Y_train_iris, Y_test_iris = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example decision tree to solve this **classification** task could look as follows:\n",
    "\n",
    "<img src='pictures/Example Decision Tree.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this decision tree fares on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode:\n",
    "    #What could be an example of a split function?\n",
    "    def __init__(self, left=None, right=None, split_fn=None, leaf_evaluate=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.split_fn = split_fn\n",
    "        self.leaf_evaluate = leaf_evaluate\n",
    "    \n",
    "    def is_leaf(self):\n",
    "        return self.left == None and self.right == None\n",
    "    \n",
    "    def evaluate(self, X_i):\n",
    "        if self.is_leaf():\n",
    "            return self.leaf_evaluate()\n",
    "        if self.split_fn(X_i):\n",
    "            return self.left.evaluate(X_i)\n",
    "        else:\n",
    "            return self.right.evaluate(X_i)\n",
    "\n",
    "#Can you trace what the evaluate function does for a tree of depth 3?\n",
    "\n",
    "class Leaf(TreeNode):\n",
    "    \n",
    "    def __init__(self, label):\n",
    "        TreeNode.__init__(self, leaf_evaluate=lambda: label)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    return (y_pred == y_true).sum() / y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, tree):\n",
    "    if len(X.shape) == 1:\n",
    "        X = X.reshape(1, -1)\n",
    "    preds = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        preds[i] = tree.evaluate(X[i])\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = TreeNode(\n",
    "    split_fn=lambda X_i: X_i[0] > 5,\n",
    "    left=TreeNode(\n",
    "        split_fn=lambda X_i: X_i[2] > 3,\n",
    "        left=Leaf(0),\n",
    "        right=Leaf(2)\n",
    "    ),\n",
    "    right=TreeNode(\n",
    "        split_fn=lambda X_i: X_i[3] > 3,\n",
    "        left=Leaf(0),\n",
    "        right=Leaf(2)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 2., 0., 0.])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(X_train_iris, root)\n",
    "preds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008333333333333333"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, Y_train_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decision tree is horrible! We didn't actually try to train it on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question is now: how do we choose how to make the splits? The answer, of course, comes from our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make things a little simpler, let's just examine the first ten data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4, 3.1, 5.5, 1.8],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [5.8, 2.6, 4. , 1.2]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_small = X_train_iris[:10]\n",
    "Y_train_small = Y_train_iris[:10]\n",
    "X_train_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 0., 2., 2., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that our first split is based on sepal length (the first feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_0 = X_train_small[Y_train_small == 0][:, 0]\n",
    "sl_1 = X_train_small[Y_train_small == 1][:, 0]\n",
    "sl_2 = X_train_small[Y_train_small == 2][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.2, 5.7])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.4, 5.2, 6. , 5.9, 5.8])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.4, 6.1, 6.4])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sl_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our decision tree, how should we split on sepal length?\n",
    "\n",
    "Just based on our training data, if we split on (Sepal Length > 6), we've isolated all irises that are class 2 (iris viginica).\n",
    "\n",
    "<img src='pictures/Simple Decision Tree.png'/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = TreeNode(\n",
    "    split_fn=lambda X_i: X_i[0] > 6,\n",
    "    left=Leaf(2),\n",
    "    right=Leaf(1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., 2., 2., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(X_train_small, simple)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, Y_train_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty good! Now let's try to come up with a programmatic way of doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind making a good decision tree is optimizing our questions (or different steps in the decision tree) to be able *to split up the data into as different categories as possible*. For example in the iris case, we would like to find a split where we may separate the various irises as much as possible. \n",
    "\n",
    "This idea of \"splitting\" to separate our irises the most introduces the idea of **entropy**. We minimize the entropy, or randomness in each split section of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='entropy'></a>\n",
    "### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's first define what entropy is. In the context of machine learning or information theory, entropy is **the measure of disorder  within a set** or the **amount of surprise**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at our training data, and the feature we chose to split on, **sepal length**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.4, 5.4, 5.2, 6.1, 6.4, 5.2, 5.7, 6. , 5.9, 5.8])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepal_length = X_train_small[:, 0]\n",
    "sepal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 0., 2., 2., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train_small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we split on (Sepal Length > 6), we divided our data into two halves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes = Y_train_small[sepal_length > 6]\n",
    "no = Y_train_small[sepal_length <= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2.])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a different split: (Sepal Length > 5.5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "yes_bad = Y_train_small[sepal_length > 5.5]\n",
    "no_bad = Y_train_small[sepal_length <= 5.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yes_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1.])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which split was better? The first, because once we made the split, *we were more sure of what class we should predict*. How can we quantify this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical definition of entropy is:\n",
    "\n",
    "$$H(\\textbf{p}) = -\\sum_i p_i \\cdot \\log_2(p_i)$$\n",
    "\n",
    "where $H(\\textbf{p})$ is equal to the entropy of the data set, and $p_i$ is the probability of getting each result.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see a visualization of the entropy of a set with two classes:\n",
    "<img src='pictures/Entropy.png' width='50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say $Pr(X = 1)$ is the probability that you flips a heads, where heads is represented by $1$ and tails is represented by $0$. From this, we can see that the y-value, $H(X)$ (or calculated entropy), is at a minimum when the chance of flipping a heads is $0$ or $1$, but is at a maximum when the chance of flipping a heads is $0.5$. In other words, the data subset is the most random when there is an equal probability of all classes, and minimized when there are probabilites of classes that are equal to $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at a set of y-values, the entropy is:\n",
    "\n",
    "$$\\sum_{\\text{class $c_i$}} -\\left(\\text{proportion of $c_i$'s}\\right) \\cdot \\log_2 \\left(\\text{proportion of $c_i$'s}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    def proportion(val, y):\n",
    "        return (y == val).sum() / len(y)\n",
    "    unique = set(y)\n",
    "    return sum(-1 * proportion(val, y) * np.log2(proportion(val, y)) for val in unique)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this comes into play in our splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_entropy = H(Y_train_small)\n",
    "original_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our good split, our entropies were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.863120568566631)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H(yes), H(no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our bad split, our entropies were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4488156357251847, 0.9182958340544896)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H(yes_bad), H(no_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the first split was better, because we reduced entropy the most.\n",
    "\n",
    "To combine these statistics together for one measure, we'll take the **weighted average**, weighting by the sizes of the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_entropy(yes, no):\n",
    "    total_size = len(yes) + len(no)\n",
    "    return (len(yes) / total_size) * H(yes) + (len(no) / total_size) * H(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4854752972273344"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H(Y_train_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6041843979966417"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_entropy(yes, no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.289659695223976"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_entropy(yes_bad, no_bad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is huge! We now have a way to choose our splits for our decision tree: \n",
    "\n",
    "**Find the best split value (of each feature) that reduces our entropy from the original set the most!**\n",
    "\n",
    "Check for understanding: why does it make sense to pick the split which reduces our entropy the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "def train(X_train, Y_train, max_depth=None):\n",
    "    if len(Y_train) == 0:\n",
    "        return Leaf(0)\n",
    "    \n",
    "    if len(set(Y_train)) == 1 or max_depth == 1:\n",
    "        return Leaf(mode(Y_train).mode)\n",
    "    \n",
    "    def split_weighted_entropy(feature_idx, feature_value):\n",
    "        feature = X_train[:, feature_idx]\n",
    "        yes = Y_train[feature > feature_value]\n",
    "        no = Y_train[feature <= feature_value]\n",
    "        return weighted_entropy(yes, no)\n",
    "    \n",
    "    splits = np.zeros(X_train.shape)\n",
    "    for feature_idx in range(X_train.shape[1]):\n",
    "        for i, feature_value in enumerate(X_train[:, feature_idx]): # try to split on each X-value\n",
    "            splits[i, feature_idx] = split_weighted_entropy(feature_idx, feature_value)\n",
    "    \n",
    "    max_idxs = X_train.argmax(axis=0)\n",
    "    for col, max_idx in enumerate(max_idxs):\n",
    "        splits[max_idx, col] = float('inf')\n",
    "    \n",
    "    i = np.argmin(splits)\n",
    "    best_feature_idx = i % splits.shape[1]\n",
    "    best_feature_value = X_train[i // splits.shape[1], best_feature_idx]\n",
    "    \n",
    "    yes = X_train[:, best_feature_idx] > best_feature_value\n",
    "    no = X_train[:, best_feature_idx] <= best_feature_value\n",
    "    \n",
    "    tree = TreeNode(\n",
    "        split_fn=lambda X_i: X_i[best_feature_idx] > best_feature_value,\n",
    "        left=train(X_train[yes], Y_train[yes], max_depth=max_depth - 1 if max_depth is not None else None),\n",
    "        right=train(X_train[no], Y_train[no], max_depth=max_depth - 1 if max_depth is not None else None)\n",
    "    )\n",
    "    \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = train(X_train_iris, Y_train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(X_train_iris, tree)\n",
    "accuracy(preds, Y_train_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa! We have a model that performs at 100% training accuracy! Let's see what happens when we try the model on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9333333333333333"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict(X_test_iris, tree)\n",
    "accuracy(preds, Y_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're doing worse, so we're probably overfitting. \n",
    "##### Question: What could be changed to avoid overfitting? (Hint: Look at parameters of our train function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use decision trees to perform regression?\n",
    "\n",
    "When we decide to make a leaf, take the mean/median of the points that are left, instead of the mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ensemble_learning'></a>\n",
    "# Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pictures/elephant.jpeg' width=\"700\" height=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the fable of the blind men and the elephant. Each individual is correct in their own right, however together their descriptions paint a much more accurate picture.\n",
    "\n",
    "We have discussed notions of bias and variance. To refresh these concepts again,  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bias** is how well the average model would perform if you trained models on many data sets.\n",
    "\n",
    "**Variance** is how different the models would be if you trained models on many data sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we only have one data set. If we train a model on this dataset, we would like to minimize both bias and variance. However, we can use some techniques to try to get the best of both worlds.\n",
    "\n",
    "Since bias is talking about how well the average model performs, and variance is about how varied the different models are, we can attempt to reduce both of these by considering an *average model*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following analogy.\n",
    "\n",
    "<img src='pictures/weather.jpg' width=\"700\" height=\"700\">\n",
    "\n",
    "We would like to predict the weather tomorrow. Perhaps we have $3$ separate sources for weather, Channel $4$ on TV, a online website, and the iPhone weather application. \n",
    "\n",
    "We may expect that a better estimate for the weather tomorrow is actually the average of all these estimates. Perhaps the different sources all have their own methods and data for creating a prediction, thus taking the average pools together all their resources into a more powerful estimator. \n",
    "\n",
    "The important gain of this approach is our improvement in variance. Keep in mind this is mentioning how different would another similar estimator be. While a single source may have high variation, such as an online website, we would expect another averaged weather amalgamation would be similar. If we considered Channel $5$ predictions, a different online website, and the Android weather application, we would not expect as much variation between their predictions since we already took the average of multiple sources. Note: for those of you who have taken a probability class, you've probably already come across this idea: taking the average generally decreases variance.\n",
    "\n",
    "Thus, one technique to improve the quality of a model is to *train multiple models on the same data and pool their predictions*. This is known as **ensembling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='bootstrapping_and_bagging'></a>\n",
    "# Bootstrapping and Bagging\n",
    "An important idea often used in data science is **bootstrapping**. This is a method to generate more samples of data, despite the fact that we only have a single dataset.\n",
    "\n",
    "**Bootstrapping**: We take the original dataset of size $N$ and draw $M$ samples with replacement of size $N$. \n",
    "\n",
    "For example, if we would like to estimate the average height of people in the U.S., we may take a sample of $1000$ people and average their heights. However, this does not tell us much about the data other than the average. We pooled together $1000$ data points into a single value, but there is much more information available. \n",
    "\n",
    "What we can do is draw many samples with replacement of size $1000$, and compute the average heights of these. This mimics as if we had many dataset, and we have many average heights. Then we can compute a distribute of average heights we would have collected, and from this we can determine how good our estimate is. By the Central Limit theorem, this distribution of bootstrapped statistics should approach the normal distribution.\n",
    "\n",
    "However, we are not limited to just calculating the average. We may calculate the median, standard deviation, or any other statistic of each bootstrapped sample. Furthermore, we can even create a model for each sample! This allows us to utilize the notion of training many models on the same dataset.\n",
    "\n",
    "If we create many models and then aggregate our predictions, this is known as **bagging** (bootstrapp aggregating). Thus we may create many separate models that all are trained on separate data to obtain new predictions and better results.\n",
    "\n",
    "The purpose of bagging is to decrease the variance of our model. Since we essentially consider many models together in parallel, we avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='random_forest'></a>\n",
    "# Random Forest\n",
    "\n",
    "A single decision tree often results in poor predictions, as they are rather simple. Just cutting the feature space into separate regions does not perform very well, as these strict linear constraints prevent complex boundaries. Furthermore, especially if you don't restrict the max depth, it's extremely prone to overfitting.\n",
    "\n",
    "However, if we include many decision trees and create a **random forest**, we obtain drastically better results. \n",
    "The idea of a random forest is quite simple, take many decision trees and output their average guess.\n",
    "<center>\n",
    "<img src=\"pictures/RandomForest.png\" width=\"60%\">\n",
    "Image from https://medium.com/@williamkoehrsen/random-forest-simple-explanation-377895a60d2d\n",
    "</center>\n",
    "\n",
    "\n",
    "In the case of classification, we take the most popular guess.   \n",
    "In the case of regression, we take some form of the average guess.\n",
    "\n",
    "\n",
    "Now, let's consider this exact setup. What would happen if we created many decision trees and took the most popular guess?\n",
    "\n",
    "In practice, we could obtain the same decision tree over and over. This is because there is some optimal set of splitting values in the dataset to minimize entropy, even with different sets of data. Perhaps we have one feature that works very well in splitting the data, and it is always utilized as the first split in the tree. Then all decision trees end up looking quite similar, despite our efforts in bagging.\n",
    "\n",
    "A solution to this problem is feature bagging. We may also select a subset of features for each tree to train on, thus each feature has a chance to be split on. \n",
    "<center>\n",
    "<img src=\"pictures/RandomForestPipeline.jpg\" width=\"40%\">\n",
    "Image from https://sites.google.com/site/rajhansgondane2506/publications\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"pictures/random_forest.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we begin with a dataset $\\mathcal{D}$ of size $N$. \n",
    "1. We bootstrap the data so that we have $M$ new datasets $d_1,\\ldots, d_M$ drawn with replacement of size $N$ from $\\mathcal{D}$.\n",
    "2. Select a subset of features $f_i$ for each new dataset $d_i$.\n",
    "3. Fit a decision tree for each $d_i$ with features $f_i$.\n",
    "\n",
    "Now to predict, we take the input data and feed it through each decision tree to get an output. Then we can take the most popular vote or the average output as the output of our model, based on the type of problem we are attempting to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "                       max_depth=10, max_features='auto', max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=200,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_iris, Y_train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model.predict(X_train_iris), Y_train_iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model.predict(X_test_iris), Y_test_iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='boosting'></a>\n",
    "# Boosting\n",
    "We have mentioned bagging as a method of decreasing variance, but what about bias? There are also techniques to do this, namely **boosting**. This is a very popular technique in Kaggle competitions and most models that win competitions utilize huge ensembles of boosted random forests.\n",
    "\n",
    "The exact implementation of boosting is out of scope for this discussion, but the main idea is to *fit your models sequentially rather than in parallel in bagging*.\n",
    "\n",
    "In random forest, we take many samples of our data, and fit separate decision trees to each one. We account for similar decision trees by feature bagging as well. However, many of these decision trees will end up predicting similar things and essentially only reduce variance.\n",
    "\n",
    "The key idea of boosting is to **emphasize the specific data points that we fail on**. Rather than trying to improve the prediction by considering the problem from different angles (e.g. new datasets from bagging or new features from feature bagging), consider where we predict incorrectly and attempt to improve our predictions from there.\n",
    "\n",
    "<center>\n",
    "<img src=\"pictures/Boosting.png\" width=\"70%\">\n",
    "Image from https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n",
    "</center>\n",
    "\n",
    "This is similar to real life. If you would like to learn a new musical piece, it is more beneficial to practice the specific part that is challenging, rather than playing the entire piece from the start every time you mess up. By boosting our model, we attempt to place greater emphasis on the samples that we consistently misclassify.\n",
    "\n",
    "\n",
    "For further reading, we highly recommend the following resources for explanations for boosting:  \n",
    "https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/  \n",
    "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d\n",
    "\n",
    "Spectacular visualizations of decision trees and boosting:  \n",
    "http://arogozhnikov.github.io/2016/06/24/gradient_boosting_explained.html\n",
    "\n",
    "There are many forms of boosting in practice. Popular ones include: Adaboost, GradientBoost, and XGBoost. XGBoost is famous (or infamous) for excelling at Kaggle competitions, many winning solutions contain XGBoost. We encourage you to look at the links above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors\n",
    "\n",
    "A natural observation to make regarding data is that data points close to one another are likely to be the same class. For example, a flower with traits similar to a rose with the exception of having slightly shorter petals or a longer stem is still very likely to still be a rose. It requires larger and more numerous changes in traits such as a color change, petal count, and stem size for that flower to be considered another variety, say a lilac.  \n",
    "\n",
    "<center>\n",
    "<img src=\"pictures/petals.png\" width=\"40%\">\n",
    "Slight variations in petal length don't change their class\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we determine the type of a flower given its specfic traits? What we can do is ask its neighbors. From our conjecture above, the class majority of the data points closest to the flower (e.g. the data points with the most similar traits) should tell us what our flower's class is. If it's near a bunch of roses, our flower is probably a rose. From here, two questions arise: \n",
    "1. How do we quantify similarity between data points?\n",
    "2. How many neighbors should we ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can answer the first question by using a distance metric, the most common being Euclidean distance.\n",
    "\n",
    "<center>\n",
    "<img src=\"pictures/euclidean.jpeg\" width=\"80%\">\n",
    "</center>\n",
    "\n",
    "We can treat each feature as a dimension and place our training data points in N-dimensional space. We can then apply the Euclidean distance metric to find the k-nearest neighbors in relation to our data point that we want to classify. After finding the $k$ nearest points, we'll use the labels of these points to decide what to label to assign to our point. For classification, we simply choose the majority class. For regression, we can take the average of these labels.\n",
    "\n",
    "A key point to note is that we have to manually specify what our hyperparamater $k$ value is. The value of $k$ will heavily influence the outcome of our model.\n",
    "\n",
    "<center>\n",
    "<img src=\"pictures/knn.png\" width=\"50%\">\n",
    "</center>\n",
    "\n",
    "It helps to think about the extreme choices of $k$ implies for k-NN. Take a moment to think what it means to set \n",
    "1. $k = 1$ (we only ask our closest neighbor)\n",
    "2. $k = N$, where $N$ is the number of data points we have. (we ask every data point)\n",
    "\n",
    "What are the bias/variance tradeoffs between both options?\n",
    "\n",
    "In the first case, we'll get a training error of 0, meaning we classify every point correctly at training time. This is because we assign each training point to its own class. As a result, our bias is incredibly low. Unfortunately, this choice of $k$ is not very robust, resulting in an extremely high variance.\n",
    "\n",
    "On the other hand, choosing $k = N$ means we simply assign each point to whatever the majority is. This gives us incredibly low variance, as new points will be assigned to the likeliest class. This also means we have incredibly high bias, as we're assigning every point to be one class and disregarding all else.\n",
    "\n",
    "Let's take a look at this in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "iris = pd.DataFrame(data= np.c_[iris['data'], iris['target']],\n",
    "                     columns= ['Sepal Length', 'Sepal Width','Petal Length','Petal Width'] + ['species'])\n",
    "\n",
    "feature_columns = ['Sepal Length', 'Sepal Width','Petal Length','Petal Width']\n",
    "X = iris[feature_columns].values\n",
    "Y = iris['species'].values\n",
    "\n",
    "#Splitting dataset into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_iris, X_test_iris, Y_train_iris, Y_test_iris = train_test_split(X, Y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy for 1-NN: 1.0\n",
      "Training accuracy for N-NN: 0.36666666666666664\n"
     ]
    }
   ],
   "source": [
    "# Code adapted from https://kevinzakka.github.io/2016/07/13/k-nearest-neighbor/\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "N = X_train_iris.shape[0]\n",
    "\n",
    "# instantiate learning model (k = 1, N)\n",
    "one_nn = KNeighborsClassifier(n_neighbors=1)\n",
    "N_nn = KNeighborsClassifier(n_neighbors=N)\n",
    "\n",
    "# fitting the model\n",
    "one_nn.fit(X_train_iris, Y_train_iris)\n",
    "N_nn.fit(X_train_iris, Y_train_iris)\n",
    "\n",
    "# predict the response\n",
    "predOne = one_nn.predict(X_train_iris)\n",
    "predN = N_nn.predict(X_train_iris)\n",
    "\n",
    "# evaluate accuracy\n",
    "print(\"Training accuracy for 1-NN: {}\".format(accuracy_score(Y_train_iris, predOne)))\n",
    "print(\"Training accuracy for N-NN: {}\".format(accuracy_score(Y_train_iris, predN)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that our 1-NN does incredibly well in training time, while our N-NN does quite poorly. In fact, if you print out the N-NN's predictions, you'll find that it predicts the same class for everything! The question then becomes: How might we choose our $k$? This leads us to our next topic: hyperparameter tuning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "\n",
    "For $k$-NNs, our main hyperparamter, which we want to find the optimal values for, is $k$. To do this, we can simply train our model at different values of the hyperparameter and compare the accuracies of the validation set to determine the best value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy for 1-NN: 1.0\n",
      "Testing accuracy for 2-NN: 0.9666666666666667\n",
      "Testing accuracy for 3-NN: 0.9666666666666667\n",
      "Testing accuracy for 4-NN: 1.0\n",
      "Testing accuracy for 5-NN: 0.9666666666666667\n",
      "Testing accuracy for 6-NN: 1.0\n",
      "Testing accuracy for 7-NN: 1.0\n",
      "Testing accuracy for 8-NN: 1.0\n",
      "Testing accuracy for 9-NN: 1.0\n",
      "Testing accuracy for 10-NN: 1.0\n",
      "Testing accuracy for 11-NN: 1.0\n",
      "Testing accuracy for 12-NN: 1.0\n",
      "Testing accuracy for 13-NN: 1.0\n",
      "Testing accuracy for 14-NN: 1.0\n",
      "Testing accuracy for 15-NN: 1.0\n",
      "Testing accuracy for 16-NN: 1.0\n",
      "Testing accuracy for 17-NN: 1.0\n",
      "Testing accuracy for 18-NN: 1.0\n",
      "Testing accuracy for 19-NN: 1.0\n",
      "Testing accuracy for 20-NN: 1.0\n",
      "Testing accuracy for 21-NN: 1.0\n",
      "Testing accuracy for 22-NN: 1.0\n",
      "Testing accuracy for 23-NN: 1.0\n",
      "Testing accuracy for 24-NN: 1.0\n",
      "Testing accuracy for 25-NN: 1.0\n",
      "Testing accuracy for 26-NN: 0.9666666666666667\n",
      "Testing accuracy for 27-NN: 0.9333333333333333\n",
      "Testing accuracy for 28-NN: 0.9666666666666667\n",
      "Testing accuracy for 29-NN: 0.9666666666666667\n",
      "Testing accuracy for 30-NN: 0.9666666666666667\n",
      "Testing accuracy for 31-NN: 0.9333333333333333\n",
      "Testing accuracy for 32-NN: 0.9\n",
      "Testing accuracy for 33-NN: 0.9333333333333333\n",
      "Testing accuracy for 34-NN: 0.9666666666666667\n",
      "Testing accuracy for 35-NN: 0.9333333333333333\n",
      "Testing accuracy for 36-NN: 0.9333333333333333\n",
      "Testing accuracy for 37-NN: 0.9333333333333333\n",
      "Testing accuracy for 38-NN: 0.9333333333333333\n",
      "Testing accuracy for 39-NN: 0.9333333333333333\n",
      "Testing accuracy for 40-NN: 0.9333333333333333\n",
      "Testing accuracy for 41-NN: 0.9333333333333333\n",
      "Testing accuracy for 42-NN: 0.9333333333333333\n",
      "Testing accuracy for 43-NN: 0.9333333333333333\n",
      "Testing accuracy for 44-NN: 0.9333333333333333\n",
      "Testing accuracy for 45-NN: 0.9333333333333333\n",
      "Testing accuracy for 46-NN: 0.9\n",
      "Testing accuracy for 47-NN: 0.9\n",
      "Testing accuracy for 48-NN: 0.8666666666666667\n",
      "Testing accuracy for 49-NN: 0.9\n",
      "Testing accuracy for 50-NN: 0.8666666666666667\n",
      "Testing accuracy for 51-NN: 0.9\n",
      "Testing accuracy for 52-NN: 0.9\n",
      "Testing accuracy for 53-NN: 0.9\n",
      "Testing accuracy for 54-NN: 0.9\n",
      "Testing accuracy for 55-NN: 0.9\n",
      "Testing accuracy for 56-NN: 0.9\n",
      "Testing accuracy for 57-NN: 0.9\n",
      "Testing accuracy for 58-NN: 0.9\n",
      "Testing accuracy for 59-NN: 0.9333333333333333\n",
      "Testing accuracy for 60-NN: 0.9\n",
      "Testing accuracy for 61-NN: 0.9\n",
      "Testing accuracy for 62-NN: 0.9\n",
      "Testing accuracy for 63-NN: 0.9\n",
      "Testing accuracy for 64-NN: 0.9\n",
      "Testing accuracy for 65-NN: 0.9\n",
      "Testing accuracy for 66-NN: 0.9\n",
      "Testing accuracy for 67-NN: 0.9\n",
      "Testing accuracy for 68-NN: 0.9\n",
      "Testing accuracy for 69-NN: 0.8\n",
      "Testing accuracy for 70-NN: 0.8\n",
      "Testing accuracy for 71-NN: 0.8\n",
      "Testing accuracy for 72-NN: 0.8333333333333334\n",
      "Testing accuracy for 73-NN: 0.8333333333333334\n",
      "Testing accuracy for 74-NN: 0.8333333333333334\n",
      "Testing accuracy for 75-NN: 0.7\n",
      "Testing accuracy for 76-NN: 0.6666666666666666\n",
      "Testing accuracy for 77-NN: 0.6666666666666666\n",
      "Testing accuracy for 78-NN: 0.6333333333333333\n",
      "Testing accuracy for 79-NN: 0.6\n",
      "Testing accuracy for 80-NN: 0.6\n",
      "Testing accuracy for 81-NN: 0.6\n",
      "Testing accuracy for 82-NN: 0.6\n",
      "Testing accuracy for 83-NN: 0.6\n",
      "Testing accuracy for 84-NN: 0.6\n",
      "Testing accuracy for 85-NN: 0.6333333333333333\n",
      "Testing accuracy for 86-NN: 0.6333333333333333\n",
      "Testing accuracy for 87-NN: 0.6\n",
      "Testing accuracy for 88-NN: 0.6\n",
      "Testing accuracy for 89-NN: 0.6\n",
      "Testing accuracy for 90-NN: 0.6\n",
      "Testing accuracy for 91-NN: 0.6\n",
      "Testing accuracy for 92-NN: 0.6\n",
      "Testing accuracy for 93-NN: 0.5666666666666667\n",
      "Testing accuracy for 94-NN: 0.5666666666666667\n",
      "Testing accuracy for 95-NN: 0.5666666666666667\n",
      "Testing accuracy for 96-NN: 0.5666666666666667\n",
      "Testing accuracy for 97-NN: 0.5666666666666667\n",
      "Testing accuracy for 98-NN: 0.5666666666666667\n",
      "Testing accuracy for 99-NN: 0.5666666666666667\n",
      "Testing accuracy for 100-NN: 0.5666666666666667\n",
      "Testing accuracy for 101-NN: 0.5666666666666667\n",
      "Testing accuracy for 102-NN: 0.5666666666666667\n",
      "Testing accuracy for 103-NN: 0.5666666666666667\n",
      "Testing accuracy for 104-NN: 0.5666666666666667\n",
      "Testing accuracy for 105-NN: 0.5666666666666667\n",
      "Testing accuracy for 106-NN: 0.5666666666666667\n",
      "Testing accuracy for 107-NN: 0.5666666666666667\n",
      "Testing accuracy for 108-NN: 0.5666666666666667\n",
      "Testing accuracy for 109-NN: 0.5666666666666667\n",
      "Testing accuracy for 110-NN: 0.5666666666666667\n",
      "Testing accuracy for 111-NN: 0.5666666666666667\n",
      "Testing accuracy for 112-NN: 0.5666666666666667\n",
      "Testing accuracy for 113-NN: 0.5666666666666667\n",
      "Testing accuracy for 114-NN: 0.5666666666666667\n",
      "Testing accuracy for 115-NN: 0.5666666666666667\n",
      "Testing accuracy for 116-NN: 0.2\n",
      "Testing accuracy for 117-NN: 0.2\n",
      "Testing accuracy for 118-NN: 0.2\n",
      "Testing accuracy for 119-NN: 0.2\n",
      "Testing accuracy for 120-NN: 0.2\n"
     ]
    }
   ],
   "source": [
    "for k in range(1, N+1):\n",
    "    k_nn = KNeighborsClassifier(n_neighbors=k)\n",
    "    k_nn.fit(X_train_iris, Y_train_iris)\n",
    "    predK = k_nn.predict(X_test_iris)\n",
    "    print(\"Testing accuracy for {}-NN: {}\".format(k, accuracy_score(Y_test_iris, predK)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Which value(s) of $k$ gives us the best training accuracy?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
