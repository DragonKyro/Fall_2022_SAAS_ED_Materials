{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-0b2745f4-065e-4d12-9ee6-70f126ea1d89",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "# Lecture 6: Linear Algebra and Linear Regression\n",
    "\n",
    "## 29 March \n",
    "\n",
    "### Table of Contents\n",
    "* [Linear Algebra](#linear_algebra)  \n",
    "    * [Motivation](#motivation)  \n",
    "    * [Notation](#notation)  \n",
    "    * [Matrix Terminology](#terminology) \n",
    "    * [Matrix Operations](#operation) \n",
    "        * [Transpose](#transpose)\n",
    "        * [Addition & Subtraction](#addsub)\n",
    "        * [Scalar Multiplication](#scalar_mul)\n",
    "        * [Matrix Multiplication](#matrix_mul)\n",
    "    * [Linear Independence](#linear_independence) \n",
    "    * [Invertibility](#invert) \n",
    "    * [Matrix Operations in numpy](#Matrix-Operations-in-numpy)\n",
    "        * [Scalar Multiplication](#Scaler-Multiplication)\n",
    "        * [Matrix Multiplication](#Matrix-Multiplication)\n",
    "        * [Matrix Inverse](#Matrix-Inverse) \n",
    "        * [Matrix Linear Independence](#Matrix-Linear-Independence) \n",
    "        * [Matrix Transpose](#Matrix-Transpose)\n",
    "\n",
    "* [Regression](#regression)\n",
    "    * [Introduction](#intro)\n",
    "        * [What Is A Model?](#what_model)\n",
    "        * [Why Make A Model?](#why_model)\n",
    "    * [Linear Regression](#linear_regression)\n",
    "        * [Simple Linear Regression](#simple)\n",
    "        * [Model Assumptions](#assumptions)\n",
    "        * [Introduction to Least Square Error](#loss)\n",
    "        * [Minimizing the Error](#ols)\n",
    "        * [Making the Model](#making_model)\n",
    "        * [Interpreting the Model](#interpreting_model)\n",
    "        * [Assessing the Model](#assessment)\n",
    "            * [Coefficient of Determination ($R^2$)](#r_squared)\n",
    "            * [Residual Plots](#residual_plots)\n",
    "            * [Q-Q Plots](#qq_plots)\n",
    "        * [When Can I Use A Linear Model?](#when_to_use)\n",
    "\n",
    "\n",
    "### Hosted by and maintained by the [Student Association for Applied Statistics (SAAS)](https://susa.berkeley.edu).\n",
    "Presented by Robert Lee, Wenhao Pan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-baf6875b-07f8-44a4-a8f1-3c8276c37777",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='linear_algebra'></a>\n",
    "# Linear Algebra\n",
    "\n",
    "<img src=\"https://economics.uwo.ca/math/resources/test-yourself-matrix-algebra/images/Matrix.png\"/>\n",
    "\n",
    "\n",
    "<a id='motivation'></a>\n",
    "## Motivation\n",
    "Linear algebra is the study of vector spaces, and it encompasses linear equations and functions represented by vector spaces and matrices.\n",
    "Vectors and matrices are essential for storing data, which is why we often use Python packages such as Numpy and Pandas.\n",
    "A common problem is linear regression, which we will delve into solving using matrices and linear algebra.\n",
    "\n",
    "\n",
    "<a id='Example'></a>\n",
    "## Example\n",
    "<img src='https://www.scdn.co/i/_global/open-graph-default.png' width='500px' />\n",
    "Spotify vectorizes their songs in order to find things that you might like. They can find the distance between some of your favorite songs and other songs by artists you haven't listened to before in order to help you discover new artists and songs.\n",
    "\n",
    "<a id='notation'></a>\n",
    "## Notation\n",
    "Common notation:\n",
    "\n",
    "* $\\mathbf{A}$: Bold capital letters represent matrices\n",
    "* $\\mathbf{x}$: Bold lowercase letters represent vectors\n",
    "* $\\theta$: Non-bold values represent scalars\n",
    "\n",
    "<a id='terminology'></a>\n",
    "## Matrix Terminology\n",
    "* **Identity** matrix: A square matrix with diagonal elements equal to $1$ and all off diagonal elements equal to zero. A $n\\times n$ identity matrix is often denoted as $I$ or $I_n$.\n",
    "* **Order or Size** of matrix: If a matrix has m rows and n columns, the order of the matrix is $m\\times n$. We denote the set of (real-valued) matrices $\\mathbb{M}_{m,n}$\n",
    "* **Transpose** of a matrix: The transpose of matrix $\\mathbf{A}$ satisfies the condition $\\mathbf{A_{j,i}} = \\mathbf{A_{i,j}}^T$. That is, the first row of $\\mathbf{A}$ is the first column of $\\mathbf{A}^T$.\n",
    "* **Square** matrix: A matrix with the same number of rows as columns. This matrix is in the shape of a square.\n",
    "* **Diagonal** matrix: A matrix with all the non-diagonal elements equal to $0$ is called a diagonal matrix.\n",
    "* **Scalar** matrix: An identity matrix multiplied by a constant.\n",
    "* **Column** matrix: A matrix which consists of exactly $1$ column. If it has $m$ rows, it can be treated as a $m\\times 1$ vector. \n",
    "* **Row** matrix: A matrix which consists of exactly $1$ row. If it has $n$ columns, it can be treated as a $1\\times n$ vector. \n",
    "* **Determinant**: A scalar value that can be computed from the elements of a square matrix and encodes certain properties of the linear transformation described by the matrix.\n",
    "* **Rank**: The maximal number of linearly independent columns of a matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-da5e269d-7610-4fff-8c73-aa352bc0b14d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "\n",
    "<a id='operation'></a>\n",
    "## Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-0a49b421-8c9d-4713-bf78-acd47247dfd2",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Several of the important things that we are going to go over how to code include scalar multiplication, matrix multiplication, matrix inverses, and matrix transposing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00005-67ed3ef0-d833-4ee0-8ab7-84a2ccbc26d0",
    "deepnote_cell_type": "code",
    "execution_millis": 3,
    "execution_start": 1603500238397,
    "output_cleared": false,
    "source_hash": "1e51c93b"
   },
   "outputs": [],
   "source": [
    "# First, let's import some useful libraries \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00006-27f48f24-6208-46c7-8b62-37bad8a4e7a1",
    "deepnote_cell_type": "code",
    "execution_millis": 11,
    "execution_start": 1603500238403,
    "output_cleared": false,
    "source_hash": "84262ea6"
   },
   "outputs": [],
   "source": [
    "# First, let's declare a couple of matrices to play with \n",
    "\n",
    "A = np.array([[-24, -18, 5],\n",
    "              [20, -15, -4],\n",
    "              [-5, 4, 1]])\n",
    "\n",
    "B = np.array([[16, -3, -8], \n",
    "              [-10, 15, 4],\n",
    "              [-9, 4, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-b550c957-fc26-46c2-9fc1-f6c4d72f5bd2",
    "deepnote_cell_type": "markdown",
    "output_cleared": false,
    "tags": []
   },
   "source": [
    "\n",
    "<a id='addsub'></a>\n",
    "### Addition & Subtraction\n",
    "if $\\mathbf{A}$ and $\\mathbf{B}$ are both $m \\times n$, we form $\\mathbf{A} + \\mathbf{B}$ by adding corresponding entries. $$\\mathbf{A} = \\begin{bmatrix} a_1 & a_2\\\\ a_3 & a_4 \\end{bmatrix}, \\mathbf{B} = \\begin{bmatrix} b_1 & b_2\\\\ b_3 & b_4 \\end{bmatrix}, \\mathbf{A} + \\mathbf{B} = \\begin{bmatrix} a_1+b_1 & a_2+b_2\\\\ a_3+b_3 & a_4+b_4 \\end{bmatrix}$$ A fact about matrix addition and transpose is that $(\\mathbf{A} + \\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T$.\n",
    "\n",
    "Similarly, we can perform subtraction in the same way, where we form $\\mathbf{A} - \\mathbf{B}$ by subtracting entries of $\\mathbf{B}$ from corresponding entries of $\\mathbf{A}$.\n",
    "\n",
    "Note that we can only perform matrix addition and subtraction when both matrices in the operation have the same dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00008-c103ad58-ba3a-4db8-807f-dc76b189ae1b",
    "deepnote_cell_type": "code",
    "execution_millis": 3,
    "execution_start": 1603501117880,
    "output_cleared": false,
    "source_hash": "8e15aa4e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -8 -21  -3]\n",
      " [ 10   0   0]\n",
      " [-14   8   2]]\n",
      "[[-40 -15  13]\n",
      " [ 30 -30  -8]\n",
      " [  4   0   0]]\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(A + B)\n",
    "\n",
    "# Subtraction\n",
    "print(A - B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-cdc7d750-5633-4b95-af6e-f78bf03098c8",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Scalar Multiplication\n",
    "\n",
    "\n",
    "<a id='scalar_mul'></a>\n",
    "### Scalar Multiplication\n",
    "We can multiply a scalar (a.k.a. number) by a matrix by multiplying every entry of the matrix by the scalar. This is denoted $\\cdot$ between the scalar and the matrix. $$c \\cdot \\begin{bmatrix} a_1 & a_2\\\\ a_3 & a_4 \\end{bmatrix} = \\begin{bmatrix} c\\times a_1 & c\\times a_2\\\\ c\\times a_3 & c\\times a_4 \\end{bmatrix}$$\n",
    "\n",
    "First, let's do an example of scalar multiplication. It works just like normal multiplication. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00008-5c06acd5-1c9a-4ed0-af43-1e64fed3cfd6",
    "deepnote_cell_type": "code",
    "execution_millis": 3,
    "execution_start": 1603500238422,
    "output_cleared": false,
    "source_hash": "921caeb8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-120,  -90,   25],\n",
       "       [ 100,  -75,  -20],\n",
       "       [ -25,   20,    5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 * A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-2df8f504-f64b-40d6-9f55-83d7ea6cd936",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Matrix Multiplication \n",
    "\n",
    "<a id='matrix_mul'></a>\n",
    "### Matrix Multiplication\n",
    "If $\\mathbf{A}$ is $m \\times p$ and $\\mathbf{B}$ is $p \\times n$, we can form $\\mathbf{C} = \\mathbf{A}\\mathbf{B}$, which has dimension $m \\times n$. For matrix multiplication to occur, the number of columns of $\\mathbf{A}$ has to equal the number of rows of $\\mathbf{B}$. A fact about matrix multiplication and transpose is that $(\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T$.$$\\begin{bmatrix} b_{11} & b_{12}\\\\ b_{21} & b_{22} \\\\ b_{31} & b_{32} \\end{bmatrix} = \\begin{bmatrix} \\mathbf{a_{1\\cdot}}\\cdot \\mathbf{b_{\\cdot1}} & \\mathbf{a_{1\\cdot}}\\cdot \\mathbf{b_{\\cdot2}}\\\\ \\mathbf{a_{2\\cdot}}\\cdot \\mathbf{b_{\\cdot1}} & \\mathbf{a_{2\\cdot}}\\cdot \\mathbf{b_{\\cdot2}} \\end{bmatrix}$$ where $\\mathbf{a_{i\\cdot}}$ is the $i$th row of matrix $\\mathbf{A}$ and $\\mathbf{b_{\\cdot j}}$ is the $j$th column of matrix $\\mathbf{B}.$\n",
    "\n",
    "For matrix multiplication, in general we don't have $\\mathbf{A}\\mathbf{B} = \\mathbf{B}\\mathbf{A}$.\n",
    "\n",
    "Next, write down matrix multiplication. Unfortunately, it is slightly different from scalar multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00010-7b40d11f-d9d9-4705-8120-44273db16639",
    "deepnote_cell_type": "code",
    "execution_millis": 4,
    "execution_start": 1603501128485,
    "output_cleared": false,
    "source_hash": "8a2792d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-249 -178  125]\n",
      " [ 506 -301 -224]\n",
      " [-129   79   57]]\n",
      "[[-404 -275   84]\n",
      " [ 520  -29 -106]\n",
      " [ 291  106  -60]]\n"
     ]
    }
   ],
   "source": [
    "# Product of A and B\n",
    "AB = np.dot(A, B)\n",
    "print(AB)\n",
    "\n",
    "# Next, try out the product of B and A. Remember, order matters! \n",
    "BA = np.dot(B, A)\n",
    "print(BA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-70d183ac-18a4-4009-bfdd-500be33828b0",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Exercise**: If $\\mathbf{A}$ is $a \\times b$ and $\\mathbf{B}$ is $b \\times c$, what are the dimensions of $(\\mathbf{A}\\mathbf{B})^T? $ How about $(\\mathbf{B}\\mathbf{A})^T$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-c2de4bae-1bb2-4c90-bd93-91ff43f15dfb",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='linear_independence'></a>\n",
    "## Linear Independence\n",
    "What does it mean for a set of vectors to be **linearly independent**? It is easier to define what it means to be linearly dependent. A set of vectors $$\\mathbf{x_1, x_2, ..., x_n}$$ is linearly dependent if there exist scalars ${\\alpha_1, \\alpha_2, ..., \\alpha_n}$, not all equal to 0 such that $$\\sum_{i=1}^n \\alpha_ix_i = 0.$$\n",
    "In words, this means that there exists at least one vector that can be written as a linear combination of the remaining vectors.\n",
    "\n",
    "\n",
    "<img src=\"https://thejuniverse.org/PUBLIC/LinearAlgebra/LOLA/indep/linIndep.png\">\n",
    "\n",
    "\n",
    "<a id='invert'></a>\n",
    "## Invertibility\n",
    "The inverse of an $n\\times n$ matrix $\\mathbf{A}$, denoted as $\\mathbf{A}^{-1}$, satisfies the following properties:\n",
    "\n",
    "$$\\mathbf{A A}^{-1} = I_{n\\times n},\\ \\mathbf{A}^{-1} \\mathbf{A}=I_{n\\times n}.$$\n",
    "We may consider a concrete example with a $2\\times 2$ matrix. \n",
    "$$\\mathbf{A} = \\begin{bmatrix} a & b\\\\ c & d\\end{bmatrix}$$\n",
    "$$\\mathbf{A}^{-1} = (\\det \\mathbf{A})^{-1}\\begin{bmatrix} d & -b\\\\ -c & a\\end{bmatrix} = \\frac{1}{ad-bc} \\begin{bmatrix} d & -b\\\\ -c & a\\end{bmatrix}$$\n",
    "\n",
    "For inverting matrices of higher dimensions, the calculation is much more difficult. We will often want to use a computer to compute these for us. But why do we care about inverse matrices in the first place? Why do they come up in linear regression?\n",
    "\n",
    "The answer to this will show up toward the end of this lecture when we consider the normal equations.\n",
    "\n",
    "Remember for later: If a matrix $\\mathbf{A}$ is invertible, so is its transpose, and the inverse of $\\mathbf{A}^T$ is the transpose of the inverse of $\\mathbf{A}$: $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00014-103a00f3-5171-4ce8-91d1-2054110bf642",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Exercise**: Are the vectors $$ \\begin{bmatrix} 1 \\\\ 2\\\\ -1 \\end{bmatrix} , \\begin{bmatrix} 0 \\\\ 1\\\\ -1 \\end{bmatrix}, \\begin{bmatrix} 1 \\\\ 1\\\\ 0 \\end{bmatrix} $$ linearly independent? \n",
    "\n",
    "**Exercise**: If the columns of a matrix are linearly independent, is the matrix invertible?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-d2db3f80-b1d4-43a3-a644-b811dce623fe",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Matrix Inverse\n",
    "Now we are going to try out inverses. We are going to be using some linear algebra functions from the numpy function. \n",
    "\n",
    "These functions are accessed by calling `numpy.linalg`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00012-03ce58ea-ad80-46e3-be46-524df8436c22",
    "deepnote_cell_type": "code",
    "execution_millis": 8,
    "execution_start": 1603501181821,
    "output_cleared": false,
    "source_hash": "74d25720"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  3.80000000e+01  1.47000000e+02]\n",
      " [-9.62193288e-17  1.00000000e+00  4.00000000e+00]\n",
      " [ 5.00000000e+00  1.86000000e+02  7.20000000e+02]]\n",
      "[[  1.  38. 147.]\n",
      " [ -0.   1.   4.]\n",
      " [  5. 186. 720.]]\n"
     ]
    }
   ],
   "source": [
    "#Try to find the inverse of matrix A. Notice how easy this is compared to doing it by hand\n",
    "\n",
    "inv = np.linalg.inv(A)\n",
    "print(inv)\n",
    "\n",
    "\n",
    "# Observe how the results are not exact. Sometimes there are rounding errors when the computer is calculating \n",
    "# the inverse of a matrix. This can resolved by rounding (you can use np.matrix.round(yourMatrixHere)) or simply \n",
    "# ignored since it is such a small error. \n",
    "print(np.round(inv))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-bba93281-c5ac-4d2d-a259-97cbd548da65",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Matrix Linear Independence \n",
    "\n",
    "However, as we learned earlier, not all matricies are invertible. How can we tell if a matrix is invertible or not? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00014-99e8c1c4-f652-4876-85a2-d1b8f5c334ff",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1603501192949,
    "output_cleared": false,
    "source_hash": "579a07e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix is invertible\n"
     ]
    }
   ],
   "source": [
    "# If a matrix's determinant is 0, it is not invertible\n",
    "\n",
    "if np.linalg.det(A) == 0:\n",
    "    print(\"Matrix is not invertible\")\n",
    "else:\n",
    "    print(\"Matrix is invertible\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-631db8c9-66ad-41fe-8516-3f4d26543641",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Matrix Transpose \n",
    "\n",
    "\n",
    "<a id='transpose'></a>\n",
    "### Transpose\n",
    "Transpose converts row vectors into column vectors, and vice versa. For example, for a $3 \\times 2$ matrix $\\mathbf{A}$ where $$ \\mathbf{A} = \\begin{bmatrix} 0 & 4\\\\ 7 & 0 \\\\ 3 & 1\\end{bmatrix}, \\mathbf{A}^T = \\begin{bmatrix} 0 & 7 & 3\\\\ 4 & 0 & 1\\end{bmatrix}$$ \n",
    "\n",
    "Note that $$(\\mathbf{A}^T)^T = \\mathbf{A}$$\n",
    "\n",
    "Lastly, let's try to find the transpose of a matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00016-a99e78db-0775-4f59-83d8-550ed95324f5",
    "deepnote_cell_type": "code",
    "execution_millis": 4,
    "execution_start": 1603500465068,
    "output_cleared": false,
    "source_hash": "425fd80c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-24,  20,  -5],\n",
       "       [-18, -15,   4],\n",
       "       [  5,  -4,   1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's transpose that matrix!!! \n",
    "A.transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00017-dc70c50a-ac90-454b-bddc-dcb13a3b9255",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='regression'></a>\n",
    "# Regression\n",
    "\n",
    "<a id='intro'></a>\n",
    "## Introduction\n",
    "Your boss is willing to do anything to get into the new hype train of *_big data_* and asks you to use your super duper important data science skills to see if there is any proof that cars that travel faster tend to cover more distance. Your boss gives you a bunch of data involving the speed of a car as well as the distance traveled by a car and it is your job to find some association. (Don't worry, at least you are getting paid well for this.)\n",
    "\n",
    "<a id='what_model'></a>\n",
    "### What Is A Model?\n",
    "A model is a simplification of the real world. We would like the make a prediction for what the real world is actually like by attempting to \"fit\" a model to the data. \n",
    "\n",
    "> Essentially, all models are wrong, but some are useful. - George Box, 1987\n",
    "\n",
    "Below are a few (3) attempts to generalize the data into a line of the form `y = a + bx`\n",
    "\n",
    "<img src='scatterAndLines.png' width=400 />\n",
    "\n",
    "\n",
    "<a id='why_model'></a>\n",
    "### Why Make A Model?\n",
    "We often find that in the real world, many problems can be (accurately) described by models created by you, the data scientist. You can make a model as an attempt to describe the relationship between many related things, such as the year a computer was made as well as how much memory it can hold. With enough data, you can begin to make predictions such as how much memory a computer may hold in 5 years (hence, the create of [Moore's Law](https://en.wikipedia.org/wiki/Moore%27s_law))\n",
    "\n",
    "However, it is important to keep in mind what you are analyzing. Just because you can make a model describing the relationship between two variables, and even if you can use this model to predict the value of one variable based on the value of the other, it doesn't mean that one causes the other. You may have heard this before as the difference between **correlation** and **causation**. A classic example is the relationship between ice cream sales and murder rates. Turns out, when ice cream sales rise, so do murder rates. Does this mean ice cream *causes* people to commit murder? Or get murdered? Nope!\n",
    "\n",
    "Today, we're going to learn how to make a **linear** model to describe the relationship between variables. A **nonlinear** model can be a decision tree and a neural network (with nonlinear activation function).\n",
    "\n",
    "<a id='linear_regression'></a>\n",
    "## Linear Regression\n",
    "**Linear regression** is a method of making linear models. Linear model is one kind of model, in which the relationship between the explanatory variables and the response variable can be described by a linear function. For now, you can just think of a linear function as a straight line, which takes us to *simple linear regression*.\n",
    "\n",
    "\n",
    "<a id='simple'></a>\n",
    "### Simple Linear Regression\n",
    "**Simple linear regression** is a special case of linear regression in which you only have one explanatory variable. As the name suggests, it models the relationship as a *line*. You may be familiar with the slope-intercept form of a line, and that's exactly how the linear model looks! \n",
    "\n",
    "$$y = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "Here, $y$ is the **response** or **dependent** variable we're trying to predict, and $x$ is an **explanatory** or **independent** variable used to predict $y$. For example, when you're trying to predict your friend's weight, $y$ would represent weight, while $x$ represents height. We are *assuming* that this is the *true* relationship between $x$ and $y$.\n",
    "\n",
    "Using known $x$'s, we want to accurately predict $y$ using the right $\\hat{\\theta}_0$ and $\\hat{\\theta}_1$. Note that here we're putting a hat on the $\\theta$'s: this is because the data sample $y_i$ we have will not be perfect (with error terms). Here, the data points we have will have some unobserved deviations from the model function above, and we model this deviation using $\\epsilon_i$ which is called *error term*. Hence, the data points we have follow\n",
    "\n",
    "$$y_i = \\theta_0 + \\theta_1x_i + \\epsilon_i$$\n",
    "\n",
    "Conventionally, we treat $x_i$'s as constants since we can observe their exact/true values. Meanwhile, we treat $\\epsilon_i$'s as random variables since we do not know their exact/true values.\n",
    "\n",
    "\\*\\* I know that seeing $\\theta$'s everywhere can be quiet scary. It is helpful to keep in mind that these variables are constants (Why?) and can be replaced with any other variable in the alphabet that makes you comfortable, such as a and b.\n",
    "\n",
    "**Question b1:** Are following models linear models?\n",
    "1. $y_i = \\theta_0 + \\theta_1x_i + \\theta_2x_i^2$\n",
    "2. $y_i = \\theta_0 + \\theta_1^2x_i$\n",
    "3. $y_i = e^{\\theta_0 + \\theta_1x_i}$\n",
    "\n",
    "<a id='assumptions'></a>\n",
    "### Model Assumptions\n",
    "Before we go into details about linear regression, let's take a look at the keys assumptions made in linear modeling.\n",
    "* **Linearity**: The relationship between the independent and response variables is linear.\n",
    "* **Homoscedasticity**: The variance of the error terms is the same for all values of the explanatory variables.\n",
    "\n",
    "<img src='Hetero.png' width=400 />\n",
    "\n",
    "* **Independence**: The error terms are independently distributed.\n",
    "* **Normality**: The error terms follow the same zero-mean normal distribution. \n",
    "\n",
    "Combining homoscedasticity and normality, we have \n",
    "$$\\forall i, \\epsilon_i \\sim N(0, \\sigma^2)$$\n",
    "\n",
    "**Question b2:** Why does each of these four assumptions imply? Why do we need them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-8d7e70e8-28e5-46e9-93ee-a288f7abe4b6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='loss'></a>\n",
    "### Introduction to Least Squared Error \n",
    "\n",
    "In the previous examples, all of the provided lines were examples of attempts to model the data. The question comes to mind: \"How do we numerically decide which line is the best line?\" As it turns out, there are many different methods that can be used to measure \"how bad\" each line is. (For those interested, it is known as a \"Loss Function,\" and more about it can be read [here](https://en.wikipedia.org/wiki/Loss_function).)\n",
    "\n",
    "The loss function that we will be teaching here is known as \"least square error.\" The general spirit of this method is that if we have a bunch of predicted values, $\\hat{y_1}, \\hat{y_2} .. \\hat{y_n}$ and a bunch of real values, $y_1, y_2 ... y_n$, we can generate a heuristic by calculating (incoming SUPER important equation) \n",
    "$$\\sum_{i = 1}^{n} \\big( y_{i} - \\hat{y}_i \\big)^{2}$$\n",
    "\n",
    "**Note:** The squaring heuristic matches the [probablistic interpretation](https://towardsdatascience.com/probabilistic-interpretation-of-linear-regression-clearly-explained-d3b9ba26823b) of linear regression. Specifically, if we use [Maximum Likelihood Estimation](https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1) (MLE), which you will learn in DATA 140 or STAT 135, then we will end up with the same square heuristic.\n",
    "\n",
    "If you remember, $\\hat{y_i}$ is our prediction, which can otherwise be expressed as $\\hat{\\theta}_0 + \\hat{\\theta}_1 x_i$. By substituting in the expression, we get \n",
    "\n",
    "$$\\sum_{i = 1}^{n} \\big( y_{i} - ( \\hat{\\theta}_0 + \\hat{\\theta}_1 x_i) \\big)^{2}$$\n",
    "\n",
    "It's great to see that we have a method of measuring how good (or bad) linear model is (by the expression above), but how do we find _the best_ model? \n",
    "\n",
    "In order to find the best model, we need to find some value for $\\hat{\\theta}_0$ and $\\hat{\\theta}_1 $ that minimizes the expression above. \n",
    "\n",
    "\n",
    "<img src='simple_linear.png' width=400 />\n",
    "\n",
    "<img src='https://www.mathsisfun.com/data/images/least-squares2.svg' />\n",
    "\n",
    "**Question b3:** What some other heuristics you can come up with? How are they different from the squaring heuristic above? \n",
    "\n",
    "**Question b4:** What are the differences between $\\theta_0, \\theta_1$ and $\\hat{\\theta}_0, \\hat{\\theta}_1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-83a80935-f1fb-4c62-8046-6f0a33b79396",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='ols'></a>\n",
    "### Minimizing the Error\n",
    "Our goal is to minimize the **sum of squared residual** or the  sum of the squared difference between the predicted value and the observed value for a given $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-7fa5d5d5-1a53-448b-995a-315192b471a6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Since we want to minimize the **residual sum of squares (RSS)**, what we're actually going to minimize is this:\n",
    "\n",
    "$$\\textit{RSS} = \\sum_{i=0}^n {e_i}^2 = \\sum_{i=0}^n (y_i - \\hat{\\theta}_0 -\\hat{\\theta}_1 x_i)^2$$\n",
    "\n",
    "By minimizing this function, we can solve for slope $\\hat{\\theta}_1 $ and the intercept $\\hat{\\theta}_0$. The actual calculations for deriving the formulas that define these coefficients requires a bit of calculus. Since calculus is a bit harder to do on the computer (it is possible!), so we'll be doing this part on paper! If you would like an online reference, feel free to click [this link](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf)! For now, we'll just tell you that $\\hat{\\theta}_1 $ and $\\hat{\\theta}_0 $ can be solved as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\hat{\\theta}_0&=\\bar {y}-\\hat{\\theta}_1\\,{\\bar{x}},\\\\\n",
    "\\hat{\\theta}_1&=\\frac{\\sum _{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar {y})}{\\sum _{i=1}^{n}(x_{i}-\\bar{x})^2}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This is a lot to digest all at once. While you do not need to memorize the proof in order to make a linear model, it is useful to understand the math behind them when you wish to understand more complicated models. [Here](http://seismo.berkeley.edu/~kirchner/eps_120/Toolkits/Toolkit_10.pdf) is a useful reference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for simple linear regression, which only has one independent variable, $\\hat{\\theta}_1$ is exactly [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-820d98f8-6d66-4442-b8df-53c3d7f7af02",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Question b5**: What will happen to $\\hat{\\theta}_0$ if we center our data, i.e. making $\\bar{y}=\\bar{x}=0$?\n",
    "\n",
    "**Question b6**: Suppose we want to use a constant model, i.e. $\\hat{y}_i = \\hat{\\theta}$ with square error, what will the $\\hat{\\theta}$ be>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00021-5c76925b-8dcd-4b00-9cbf-3ccffb57f430",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='making_model'></a>\n",
    "### Making the Model\n",
    "In linear regression, the response variable is usually continuous. The explanatory variables *can* be discrete and even categorical, but for simple linear regression they are usually continuous. For today we'll just be working with continuous variables! \n",
    "\n",
    "Let's work with the `mpg` dataset and decide whether it's appropriate for making a simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00022-0a9170e7-9a1f-4304-870d-87f98b0e718e",
    "deepnote_cell_type": "code",
    "execution_millis": 17,
    "execution_start": 1603500238453,
    "output_cleared": false,
    "source_hash": "8e03daf6"
   },
   "outputs": [],
   "source": [
    "#Some useful imports \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from plotting import overfittingDemo, plot_multiple_linear_regression\n",
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00023-ccdbf088-ed74-41cd-a81d-06ff55cfde43",
    "deepnote_cell_type": "code",
    "execution_millis": 3,
    "execution_start": 1603500238502,
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "c8b6190e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toyota corolla</th>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "      <td>70</td>\n",
       "      <td>2245</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buick century</th>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>231</td>\n",
       "      <td>110</td>\n",
       "      <td>3907</td>\n",
       "      <td>21</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cadillac eldorado</th>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>350</td>\n",
       "      <td>125</td>\n",
       "      <td>3900</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmw 320i</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>121</td>\n",
       "      <td>110</td>\n",
       "      <td>2600</td>\n",
       "      <td>12</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ford fairmont futura</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>140</td>\n",
       "      <td>92</td>\n",
       "      <td>2865</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      mpg  cylinders  displacement  horsepower  weight  \\\n",
       "name                                                                     \n",
       "toyota corolla         34          4           108          70    2245   \n",
       "buick century          17          6           231         110    3907   \n",
       "cadillac eldorado      23          8           350         125    3900   \n",
       "bmw 320i               21          4           121         110    2600   \n",
       "ford fairmont futura   24          4           140          92    2865   \n",
       "\n",
       "                      acceleration  model_year  origin  \n",
       "name                                                    \n",
       "toyota corolla                  16          82       3  \n",
       "buick century                   21          75       1  \n",
       "cadillac eldorado               17          79       1  \n",
       "bmw 320i                        12          77       2  \n",
       "ford fairmont futura            16          82       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpg = pd.read_csv(\"./mpg.csv\", index_col=\"name\") # load mpg dataset\n",
    "mpg = mpg.loc[mpg[\"horsepower\"] != '?'].astype(int) # remove rows with missing horsepower values\n",
    "mpg_train, mpg_test = train_test_split(mpg, test_size = .2, random_state = 0) # split into training set and test set\n",
    "mpg_train, mpg_validation = train_test_split(mpg_train, test_size = .5, random_state = 0) \n",
    "mpg_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-200106c6-6e4d-4072-b278-55662835be5a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Here we've chosen the `mpg` dataset, which tells us various attributes of different cars, including a car's make and model, miles per gallon, number of cylinders, weight, and more! We're going to be trying to see which features affect a car's `mpg`, and our goal is to create a model that accurately predicts `mpg` given other attributes of the car. \n",
    "\n",
    "You'll notice that we separated the `mpg` data into two separate dataframes, `mpg_train` and `mpg_test`. We'll get into why in next lecture, but for now, make sure to do all of your analysis and model creation on the `mpg_train` dataset! \n",
    "\n",
    "\n",
    "*Hint:* Hitting `shift-tab` with the cursor on the name of a function will bring up helpful documentation about how to use the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00025-3ab9cfc9-ca18-4737-b9ba-4cc80aae7499",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Since we are looking at the data for cars, what could be a useful variable to predict?\n",
    "Furthermore, to predict this variable, what explanatory or response variable should we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00026-186803dd-d748-43d2-9075-d8319840a808",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1603500238508,
    "output_cleared": false,
    "source_hash": "953e03ac"
   },
   "outputs": [],
   "source": [
    "x1 = ...\n",
    "y1 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00027-075ef2ca-0881-4b0f-bcb0-05cea3365012",
    "deepnote_cell_type": "code",
    "execution_millis": 1,
    "execution_start": 1603500238531,
    "output_cleared": false,
    "source_hash": "3192c4aa"
   },
   "outputs": [],
   "source": [
    "mpg_train.plot.scatter(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-a3cc5d5f-e0b3-45db-971e-12623eb119d0",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "`sklearn`'s `linear_model` module makes it really easy to make linear models! There's a lot of different types of linear models implemented in the `linear_model` module, which you can take a look at [here](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) if you're interested, but for today we'll be using `LinearRegression`, which we've imported for you in the cell below. Try reading the documentation to figure out what the `fit()` function expects as input to correctly fit our model to the `mpg_train` data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00029-b0008c58-8ee4-4d68-ad07-3aa3c0c096b1",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "9527aab5"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00030-d2c2eb45-f091-452c-8ae7-ba14318205d1",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "ceb3afb3"
   },
   "outputs": [],
   "source": [
    "# Initialize our linear regression model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "X = ...\n",
    "y = ...\n",
    "\n",
    "# Fit the model to the data\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-a50a4be6-59bf-4cc6-96b8-28df2177c6dc",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Once you've got it working you'll notice that it seems like nothing happened. However, behind the scenes, our `linear_model` variable has now been fit to the data we passed into the `fit()` function! We can see what the `slope` and `intercept` are by looking into the `coef_` and `intercept_` attributes of our `linear_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-2ba87ae1-db8b-489c-9c39-c2e337d713a4",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "7b8db49d"
   },
   "outputs": [],
   "source": [
    "linear_model.coef_, linear_model.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-58213063-3f75-409e-8e7d-672e9c9da797",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "You might notice that, while the `intercept_` is a single scalar value, `coef_` returns an array. This is because you can choose to fit your model to multiple explanatory variables (hence the list form of `feature_cols`). When you define multiple explanatory variables, the `coef_` will contain a separate coefficient for each explanatory variable you chose! You'll be able to explore that in a bit, but for now let's take a look at what our linear model looks like relative to our original data.\n",
    "\n",
    "We've provided the skeleton for a helper function called `overlay_simple_linear_model`. Try to fill out the function so that it plots a scatterplot with the linear model overlaid on top.\n",
    "\n",
    "*Hint:* If you press `tab` after a `[object].` or `[package].`, Jupyter will show you a list of valid functions defined for that object type or package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00034-e20a2db6-cc6a-4b9c-b567-65b753df4f3f",
    "deepnote_cell_type": "code",
    "execution_millis": 5,
    "execution_start": 1603501361985,
    "output_cleared": false,
    "source_hash": "45c53cca"
   },
   "outputs": [],
   "source": [
    "def overlay_simple_linear_model(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a simple linear model on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model: a fitted linear model\n",
    "    \n",
    "    returns None but outputs linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.arange(max(data[x_name])).reshape(-1, 1) # a 2D array of integers between 0 and the maximum value of the x_name column\n",
    "    y = linear_model.predict(...) # use linear_model to predict\n",
    "    \n",
    "    data.plot.scatter(...) # scatter plot of x_name vs. y_name\n",
    "    \n",
    "    plt.plot(...)\n",
    "    plt.title(\"Linear Model vs. Data: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00035-940fca73-2eb2-4eef-b40e-ec9da14d5cbc",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "3ba1deeb"
   },
   "outputs": [],
   "source": [
    "# If you wrote the function above correctly, the output should look like this\n",
    "overlay_simple_linear_model(mpg_train, '...', \"mpg\", linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00036-cf5ad3af-e105-4b73-ba05-94678ed252b6",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='interpreting_model'></a>\n",
    "### Interpreting the Model\n",
    "\n",
    "You're probably thinking \"COOL! This looks like a pretty good representation of the data! But what do these coefficients even mean?\" That is a great question! As you might have guessed, the `intercept` term is where our line intersects with the y-axis, or when our predictor variable has a value of 0. In relation to our model, it's our prediction for `mpg` given a predictor variable value of 0. The `slope` term is a little more complicated. Yes, it is the slope of the line, but how do we interpret it in the relationship between `mpg` and our explanatory variable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00037-ed452b7a-6186-4ab8-a314-021df34cb435",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='assessment'></a>\n",
    "### Assessing the Model\n",
    "<a id='r_squared'></a>\n",
    "#### Coefficient of Determination ($R^2$)\n",
    "Another question you might have is, how do we know how good our model is?\n",
    "If you've taken AP Stats or Data 8, you might have heard that the **correlation coefficient**, $R$, \n",
    "tells us how strong of an association two variables have. Values close to -1 or 1 have a strong association.\n",
    "We are going to talk about another way of measuring how well your model fits the data is the $R^2$ coefficient,\n",
    "or the **coefficient of determination**. Basically, what the $R^2$ represents is how much our data can vary but\n",
    "still be predicted accurately by the explanatory variable. \n",
    "$$R^2 = 1- \\frac{\\sum_{i}e_i^2}{\\sum_{i}(y_i-\\bar{y})^2}$$\n",
    "where $\\sum_{i}(y_i-\\bar{y})^2$ is called **total sum of squares** and $\\sum_{i}e_i^2$ is called **residual sum of squares** as we've already known. Intuitively, $R^2$ measures how much better our fitted model is doing compared to the most basic model where we\n",
    "predict $\\hat{y}_i = \\bar{y}$ for every data point.\n",
    "\n",
    "<img src='https://i.imgur.com/URDr1oW.png' />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00038-92ca540a-55d2-4afb-804e-c030b8fd27ac",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "We can obtain our model's $R^2$ value by using our `linear_model`'s `score()` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00039-ca4b585a-f0b8-4202-ad3d-cbc47bc86833",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "2f8bb85e"
   },
   "outputs": [],
   "source": [
    "linear_model.score(...) # you'll only need to use variables that we've already defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-284acd34-8f33-4adb-8391-881cf39c61fc",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Woohoo! If you used `displacement`, our model accurately predicts 66% of the variation in `mpg`. In practice, $R^2$ is almost always between 0 and 1, although it is possible for $R^2$ to take on a negative value when your model is worse than the baseline model $\\hat{y}_i = \\bar{y}$.\n",
    "\n",
    "**Question b7:** What does it mean for $R^2$ to have a value of 1? What about 0?\n",
    "\n",
    "\n",
    "**Question b8:** Can you think of a possible feature you could use to make our model have an $R^2$ value of $1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00041-8230c8bc-e715-46fb-a06e-4890dff719a9",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "d8444c1a"
   },
   "outputs": [],
   "source": [
    "linear_model2 = LinearRegression()\n",
    "\n",
    "X2 = ...\n",
    "\n",
    "linear_model2.fit(X2, y)\n",
    "\n",
    "r_squared = ...\n",
    "\n",
    "r_squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00042-8279e267-8b5a-4973-be3b-d7825bdebd5d",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='residual_plots'></a>\n",
    "#### Residual Plots\n",
    "Another way of analyzing your model is through *residual plots*. A **residual plot** is kind of what you'd think â€“ it plots your residuals against the corresponding $x$ values. If you see interesting patterns in your residual plot, it's indicative of some *bias* in your model â€“ your error isn't due to randomness in the data but because of an underlying problem in the way you've defined the relationship between your variables. \n",
    "\n",
    "Fill in the blanks in the `plot_simple_residuals()` function, so we can take a look at the residual plot.\n",
    "\n",
    "\n",
    "\n",
    "<img src='https://www.qualtrics.com/support/wp-content/uploads/2017/07/Screen-Shot-2017-07-19-at-9.46.11-AM.png' width='800px' />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00043-541fb3a8-e86c-4739-8369-c82f118aabcf",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "efdfd02"
   },
   "outputs": [],
   "source": [
    "def plot_simple_residuals(data, x_name, y_name, linear_model):\n",
    "    \"\"\"\n",
    "    This function plots a residual plot based off of a simple linear model \n",
    "    on top of the scatterplot of the data it was fit to.\n",
    "    \n",
    "    data(DataFrame): e.g. mpg_train\n",
    "    x_name(string): the name of the column representing the predictor variable\n",
    "    y_name(string): the name of the column representing the dependent/response variable\n",
    "    linear_model\n",
    "    \n",
    "    returns None but outputs residual plot resulting from linear model overlaid on scatterplot\n",
    "    \"\"\"\n",
    "    X = ...\n",
    "    y = ...\n",
    "    residuals = ...\n",
    "    \n",
    "    ... # plot residuals (Hint: use plt.scatter())\n",
    "    plt.axhline(y=0, color='r', linestyle='-') # plots line at y = 0\n",
    "    plt.title(\"Residual Plot: \" + x_name + \" vs. \" + y_name)\n",
    "    plt.xlabel(x_name)\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00044-a3bcf9d8-793d-4c87-b3ef-8c1864e5af41",
    "deepnote_cell_type": "code",
    "output_cleared": false,
    "source_hash": "b10131b1"
   },
   "outputs": [],
   "source": [
    "plot_simple_residuals(mpg_train, ..., 'mpg', linear_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00045-c60c38e1-89ca-48de-8bc9-8378e5d93469",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "What do you observer from the residual plot? [Here](https://www.qualtrics.com/support/stats-iq/analyses/regression-guides/interpreting-residual-plots-improve-regression/)'s some more information about how to interpret different patterns in residual plots and how you can change your model to fix these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b9:** What model assumption(s) we made previously tell us that we want the residuals of our model look like *randomly scattered* around $y=0$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-34efa03f-27d3-441b-9e08-5f7ba00b8f2f",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='qq_plots'></a>\n",
    "#### Q-Q Plots\n",
    "A **Q-Q plot** (quantile-quantile plot) is a scatterplot created by plotting two sets of quantiles against one another. If both sets of quantiles came from the same distribution, we should see the points forming a line thatâ€™s roughly straight. Quantiles split up a distribution into equal-sized subgroups. For example, a median splits a distribution into 2 equl areas, so it is called a 2-quantile. You can read more about quantiles [here](https://www.ucd.ie/ecomodel/Resources/QQplots_WebVersion.html).\n",
    "\n",
    "Here's an example of a Normal Q-Q plot when both sets of quantiles truly come from standard normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-6f153a3c-c4f0-410e-a683-2dbce59f6342",
    "deepnote_cell_type": "code",
    "execution_millis": 340,
    "execution_start": 1603502111680,
    "output_cleared": false,
    "scrolled": true,
    "source_hash": "c478b059"
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "nsample = 100\n",
    "np.random.seed(134)\n",
    "x = stats.norm.rvs(loc=0, scale=1, size=nsample) # generating 100 data points from a standard normal distribution\n",
    "fig = sm.qqplot(x, line = '45') # plot the theoretical line where all points should be close to\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00048-9a2e3ae5-48fd-4ce0-9053-8e0b02a6a9b4",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "Q-Q plot is very useful for checking the normality assumption of linear regression: by plotting the distribution of the data at hand against the distribution of the ideal normal variable, we are able to observe how closely our data follows a normal distribution. If roughly all the points lie on the 45$^{\\circ}$ line, then we assume the normality assumption is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question b10**: What will happen if we are plotting quantiles of some other distributions against quantiles of standard normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00049-b2bd918c-d312-456a-9c74-250096d4feb1",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "<a id='when_to_use'></a>\n",
    "### When is a Linear Model a good fit for my data?\n",
    "Let's talk some more about the assumptions of linear regression, so you know when it's appropriate to use. Fill in the blanks: \n",
    "- There's a ________________ relationship between the response variable and the explanatory variables.\n",
    "- There's ____ pattern in the residual plot.\n",
    "- Each data point collected must be ____ of each other. \n",
    "- The errors and the response variable are normally distributed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00053-6bb09461-d97a-40cb-87b0-d31cc00b11e9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Is it good or bad if there's a linear pattern in your residuals?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00050-b80bd02a-e2fc-427d-a95f-9d1247769c23",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Adding Explanatory Variables\n",
    "Now that you're a master of simple linear regression, you're probably thinking \"WHY CAN'T I USE MORE EXPLANATORY VARIABLES? What if I think `mpg` could be better predicted if I knew *two* of the variables? Wouldn't that make my model better?\" Why, Ms/Mr. Genius Statistician, you *can* use more explanatory variables! That leads us to *multiple linear regression*, which you will see next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00051-a25af081-bc1d-4568-b500-cfed0f48ed1a",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "### Matrix Form of Simple Linear Regression\n",
    "Now let's put everything together and put everything in the matrix form. $$\\mathbf{\\hat{y}} = \\begin{bmatrix} \\hat{y}_1 \\\\ \\hat{y}_2\\\\ \\vdots \\\\ \\hat{y}_n\\end{bmatrix}, \\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ 1 & x_2\\\\ \\vdots \\\\ 1 & x_n\\end{bmatrix}, \\mathbf{\\hat{\\theta}} = \\begin{bmatrix} \\hat{\\theta}_0 \\\\ \\hat{\\theta}_1\\end{bmatrix}$$ $$\\mathbf{\\hat{y}} = \\mathbf{X}\\mathbf{\\hat{\\theta}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework questions\n",
    "* Linear Regression: Finish all the lecture questions (e.g. Question b2) and replace `\"YOUR ANSWER HERE\"` strings with your answers below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qb1 = \"YOUR ANSWER HERE\"\n",
    "Qb2 = \"YOUR ANSWER HERE\"\n",
    "Qb3 = \"YOUR ANSWER HERE\"\n",
    "Qb4 = \"YOUR ANSWER HERE\"\n",
    "Qb5 = \"YOUR ANSWER HERE\"\n",
    "Qb6 = \"YOUR ANSWER HERE\"\n",
    "Qb7 = \"YOUR ANSWER HERE\"\n",
    "Qb8 = \"YOUR ANSWER HERE\"\n",
    "Qb9 = \"YOUR ANSWER HERE\"\n",
    "Qb10 = \"YOUR ANSWER HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00052-17717b6b-eed7-4344-b307-e652774452d4",
    "deepnote_cell_type": "markdown",
    "output_cleared": false
   },
   "source": [
    "## Resources/References\n",
    "- [Matplotlib Tutorial - Nicolas P. Rougier](https://www.labri.fr/perso/nrougier/teaching/matplotlib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=4d2595a8-48cb-4553-85ca-2d6076eb07b1' target=\"_blank\">\n",
    "<img style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "a17cbbb6-ba38-45d9-90dc-16fdc0826773",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
