{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why Might You Webscrape?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're faced with a data science project, you may want to use outside data resources found on the web. For example, you may want to use news headlines for sentiment analysis, historical weather data for a forecasting project, or online shop prices for your personal product sniper. However, although many websites present data in a pleasant, easy-to-read manner, it is often not downloadable, making it difficult to use the data in a data science project.  What to do now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's where webscraping comes in. If you can webscrape, you don't necessary need the data to be downloadable. You just need the data to be presented in a neat, organized manner and your webscraper will take care of the rest. Let's take a look at some examples below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most python projects, webscraping will require you download/install a few libraries. The first is the `requests` library. This library will help you access data from websites given a specific URL. The second library is the `BeautifulSoup` library. This library will help you parse and read the data. The final library is the `pandas` library which you may already be familiar with. This library will help you organize the data and export it to your customary csv file. To import the libraries, run the import commands below\n",
    "\n",
    "If you receive an error along the lines of \"No Module named BLANK\", try running:\n",
    "- `!pip install requests`\n",
    "- `!pip install beautifulsoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News Headlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will scrape news headlines from https://www.nbcnews.com/business.  Feel free to visit the website and take a look at the weather data available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, in the code cell below, we are defining the URL we wish to scrape and then using `requests.get()` to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbc_business = \"https://www.nbcnews.com/business\"\n",
    "res = requests.get(nbc_business)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the result from `requests.get()` and feed it into `BeautifulSoup()` to create an object we can parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this time, it would be a good idea to turn our attention back to the website we're scraping. On the news site, hover above a headline, right click, and click Inspect. You should see HTML code pop up with a line highlighted. The HTML code represents the source code for the webpage we're on and the highlighted portion represents the code specifically for the news headline. Pretty neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not super important to understand what's going on with the HTML right now. Just know that there are tags such as `<span> </span>` and attributes such as `class = \"tease-card__headline\"`. For webscraping purposes, you want to find these tags and identifying attributes because we can parse the data using the `find_all()` function like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = soup.find_all('span', {'class': 'tease-card__headline'})\n",
    "headlines += soup.find_all('h2', {'class': 'wide-tease-item__headline'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have grabbed all of our news headlines, we only need to put them into a pandas dataframe and create a csv. The following code cell does just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nbc = pd.DataFrame(headlines)\n",
    "data_nbc.to_csv(\"headlines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Airbnb changes booking process for travelers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In Georgia’s midterms, a growing health care c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How American families are dealing with the new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A recession is coming, economists say. When it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Evictions are piling up across the U.S. as Cov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A seismic grocery merger faces major oppositio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hong Kong is inviting back the (business) worl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Inside Twitter's chaotic short-notice layoffs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Advertisers pull back from Twitter amid 'uncer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How pay transparency laws set the stage for an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Twitter sued over short-notice layoffs as Elon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>As major companies shut down stores with union...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>U.S. economy added 261,000 jobs in October, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Elon Musk has begun laying off Twitter staffer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    0\n",
       "0       Airbnb changes booking process for travelers \n",
       "1   In Georgia’s midterms, a growing health care c...\n",
       "2   How American families are dealing with the new...\n",
       "3   A recession is coming, economists say. When it...\n",
       "4   Evictions are piling up across the U.S. as Cov...\n",
       "5   A seismic grocery merger faces major oppositio...\n",
       "6   Hong Kong is inviting back the (business) worl...\n",
       "7       Inside Twitter's chaotic short-notice layoffs\n",
       "8   Advertisers pull back from Twitter amid 'uncer...\n",
       "9   How pay transparency laws set the stage for an...\n",
       "10  Twitter sued over short-notice layoffs as Elon...\n",
       "11  As major companies shut down stores with union...\n",
       "12  U.S. economy added 261,000 jobs in October, be...\n",
       "13  Elon Musk has begun laying off Twitter staffer..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_nbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to try scraping weather data, specifically the weather of Berkeley. Please visit https://forecast.weather.gov/MapClick.php?lat=37.8699&lon=-122.2705#.Y2n19eTMJD8 to see the site we are scraping. Again, the following code cell will grab the data from the appropriate URL and then build a parser using BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://forecast.weather.gov/MapClick.php?lat=37.8699&lon=-122.2705#.Y2n19eTMJD8\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we wish to grab multiple columns of data. More specifically, we wish to grab the time period, a short description of the weather, and the extreme temperature. If you take a look at the HTML code, you'll see that the data we want is nested inside a `<div>` tag identified by the attribute `class = tombstone-container`. The following code block first finds all the `tombstone-container` and then grabs the `period-name`, `short-desc` and `temp` attributes nested within each container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "items = soup.find_all(\"div\", {\"class\": \"tombstone-container\"})\n",
    "period_name = [item.find(\"p\", {\"class\": \"period-name\"}).get_text() for item in items]\n",
    "short_desc = [item.find(\"p\", {\"class\": \"short-desc\"}).get_text() for item in items]\n",
    "temp = [item.find(\"p\", {\"class\": \"temp\"}).get_text() for item in items]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to convert the scraped data into a meaningful dataframe and csv. The code cell below organizes all of our data and outputs a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Period\" : period_name, \"Short Description\" : short_desc, \"Temperature\" : temp})\n",
    "df.to_csv(\"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Period</th>\n",
       "      <th>Short Description</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tonight</td>\n",
       "      <td>Showers</td>\n",
       "      <td>Low: 51 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Showers</td>\n",
       "      <td>High: 54 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TuesdayNight</td>\n",
       "      <td>ShowersLikely</td>\n",
       "      <td>Low: 49 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wednesday</td>\n",
       "      <td>ChanceShowers</td>\n",
       "      <td>High: 54 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WednesdayNight</td>\n",
       "      <td>Partly Cloudy</td>\n",
       "      <td>Low: 42 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Thursday</td>\n",
       "      <td>Mostly Sunny</td>\n",
       "      <td>High: 55 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ThursdayNight</td>\n",
       "      <td>Mostly Clear</td>\n",
       "      <td>Low: 40 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>VeteransDay</td>\n",
       "      <td>Partly Sunny</td>\n",
       "      <td>High: 55 °F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FridayNight</td>\n",
       "      <td>Mostly Cloudy</td>\n",
       "      <td>Low: 42 °F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Period Short Description  Temperature\n",
       "0         Tonight           Showers   Low: 51 °F\n",
       "1         Tuesday           Showers  High: 54 °F\n",
       "2    TuesdayNight     ShowersLikely   Low: 49 °F\n",
       "3       Wednesday     ChanceShowers  High: 54 °F\n",
       "4  WednesdayNight     Partly Cloudy   Low: 42 °F\n",
       "5        Thursday      Mostly Sunny  High: 55 °F\n",
       "6   ThursdayNight      Mostly Clear   Low: 40 °F\n",
       "7     VeteransDay      Partly Sunny  High: 55 °F\n",
       "8     FridayNight     Mostly Cloudy   Low: 42 °F"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webscrape Online Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our last exercise, we will scrape an online storefor prices, ratings, and product description. We will be scraping https://www.thewhiskyexchange.com if you would like to take a gander. The code cell below defines the URL to visit as well as a header for our requests. This header is important because sometimes, websites will try to block suspicious consecutive requests such as webscraping. In order to appear like a normal user, the header defines certain fields to that are filled out when normally browsing on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.thewhiskyexchange.com\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this webscraping exercise, there are actually multiple URL's we want to visit, namely one for each page of products. Additionally, we don't just want to visit the initial search page. We actually want to visit each individual product's page and grab the relevant data. In order to do this, we first scrape for the relevant links using the for loop below. Notice how we're actually changing the URL for each iteration of the for loop to scrape a different page of products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "productlinks = []\n",
    "t = {}\n",
    "data = []\n",
    "c = 0\n",
    "for x in range(1,6):\n",
    "    k = requests.get('https://www.thewhiskyexchange.com/c/35/japanese-whisky?pg={}&psize=24&sort=pasc'.format(x)).text\n",
    "    soup = BeautifulSoup(k, 'html.parser')\n",
    "    productlist = soup.find_all(\"li\", {\"class\": \"product-grid__item\"})\n",
    "\n",
    "    for product in productlist:\n",
    "        link = product.find(\"a\", {\"class\": \"product-card\"}).get('href')\n",
    "        productlinks.append(baseurl + link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of product URLs, we can scrape for the data for each product. The following for loop does just that, scraping for the product's price, description, name, and overview and then saving them to a dictionary. Note, this code cell may take a while to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in productlinks:\n",
    "    f = requests.get(link,headers=headers).text\n",
    "    hun=BeautifulSoup(f,'html.parser')\n",
    "\n",
    "    try:\n",
    "        price=hun.find(\"p\", {\"class\": \"product-action__price\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        price = None\n",
    "\n",
    "    try:\n",
    "        about=hun.find(\"div\", {\"class\": \"product-main__description\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        about=None\n",
    "\n",
    "    try:\n",
    "        rating = hun.find(\"div\", {\"class\": \"review-overview\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        rating=None\n",
    "\n",
    "    try:\n",
    "        name=hun.find(\"h1\", {\"class\": \"product-main__name\"}).text.replace('\\n',\"\")\n",
    "    except:\n",
    "        name=None\n",
    "\n",
    "    whisky = {\"name\": name, \"price\": price, \"rating\": rating, \"about\": about}\n",
    "\n",
    "    data.append(whisky)\n",
    "    c = c + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's organize all of our data and output to a csv. Running the following code cell will do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"prices.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>about</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Suntory Toki</td>\n",
       "      <td>£33.95</td>\n",
       "      <td>4(34 Reviews)</td>\n",
       "      <td>Toki is a blended whisky from Suntory's three ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shinju Japanese Whisky</td>\n",
       "      <td>£37.45</td>\n",
       "      <td>None</td>\n",
       "      <td>A sweet, fruity Japanese whisky from Shinju th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Chita Whisky</td>\n",
       "      <td>£51.95</td>\n",
       "      <td>4.5(42 Reviews)</td>\n",
       "      <td>Chita is Suntory's grain distillery. The flags...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nikka Coffey Grain Whisky</td>\n",
       "      <td>£57.95</td>\n",
       "      <td>4.5(52 Reviews)</td>\n",
       "      <td>A release of grain whisky from Japan's Nikka, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shinju 8 Year Old Japanese Whisky</td>\n",
       "      <td>£59.75</td>\n",
       "      <td>None</td>\n",
       "      <td>A sweet, fruity eight-year-old whisky from Shi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>Yamazaki 12 Year Old</td>\n",
       "      <td>£150</td>\n",
       "      <td>4.5(94 Reviews)</td>\n",
       "      <td>One of the first Japanese single malts to brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>Akashi 5 Year OldRed Wine Cask</td>\n",
       "      <td>£160</td>\n",
       "      <td>None</td>\n",
       "      <td>A single malt from the White Oak distillery, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Mars Komatgatake IPA FinishBot.2021</td>\n",
       "      <td>£175</td>\n",
       "      <td>None</td>\n",
       "      <td>A limited-edition Mars Komagatake Japanese sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Shizuoka Contact S Single Malt3 Year Old</td>\n",
       "      <td>£175</td>\n",
       "      <td>None</td>\n",
       "      <td>The third whisky to come out of Shizuoka disti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>Hakushu 12 Year Old</td>\n",
       "      <td>£185</td>\n",
       "      <td>4.5(32 Reviews)</td>\n",
       "      <td>A perennial favourite and a must-try for anyon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name   price           rating  \\\n",
       "0                                Suntory Toki  £33.95    4(34 Reviews)   \n",
       "1                      Shinju Japanese Whisky  £37.45             None   \n",
       "2                            The Chita Whisky  £51.95  4.5(42 Reviews)   \n",
       "3                   Nikka Coffey Grain Whisky  £57.95  4.5(52 Reviews)   \n",
       "4           Shinju 8 Year Old Japanese Whisky  £59.75             None   \n",
       "..                                        ...     ...              ...   \n",
       "115                      Yamazaki 12 Year Old    £150  4.5(94 Reviews)   \n",
       "116            Akashi 5 Year OldRed Wine Cask    £160             None   \n",
       "117       Mars Komatgatake IPA FinishBot.2021    £175             None   \n",
       "118  Shizuoka Contact S Single Malt3 Year Old    £175             None   \n",
       "119                       Hakushu 12 Year Old    £185  4.5(32 Reviews)   \n",
       "\n",
       "                                                 about  \n",
       "0    Toki is a blended whisky from Suntory's three ...  \n",
       "1    A sweet, fruity Japanese whisky from Shinju th...  \n",
       "2    Chita is Suntory's grain distillery. The flags...  \n",
       "3    A release of grain whisky from Japan's Nikka, ...  \n",
       "4    A sweet, fruity eight-year-old whisky from Shi...  \n",
       "..                                                 ...  \n",
       "115  One of the first Japanese single malts to brea...  \n",
       "116  A single malt from the White Oak distillery, r...  \n",
       "117  A limited-edition Mars Komagatake Japanese sin...  \n",
       "118  The third whisky to come out of Shizuoka disti...  \n",
       "119  A perennial favourite and a must-try for anyon...  \n",
       "\n",
       "[120 rows x 4 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's all from us folks. Today, we have scraped news headlines, weather data, and online shopping prices. Hopefully by now you are a web scraping expert. Now have fun scraping the web and using this skill to your advantage. Remember to scrape responsibly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://shailchudgar005.medium.com/web-scraping-using-python-weather-data-71a8194d9c01\n",
    "- https://python.plainenglish.io/news-headlines-web-scrapper-4f4fdbf87c1e\n",
    "- https://www.freecodecamp.org/news/scraping-ecommerce-website-with-python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
