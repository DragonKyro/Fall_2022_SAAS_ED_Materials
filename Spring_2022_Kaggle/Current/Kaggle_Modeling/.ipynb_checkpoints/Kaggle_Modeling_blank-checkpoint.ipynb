{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CX Kaggle Competition: Salary Prediction\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "* [5. Setup](#setup)\n",
    "* [6. Validation](#validation)\n",
    "* [7. Accuracy & Error](#rmse)\n",
    "* [8. Different Models](#models)\n",
    "* [9. Submission](#submission)\n",
    "\n",
    "### Hosted by and maintained by the [Students Association of Applied Statistics (SAAS)](https://saas.berkeley.edu). Authored by Jasmine Lee and Akhil Vemuri. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "We will now be creating various regression models in order to predict the ```totalyearlycompensation``` column of our data. There are multiple different parts to manage when modeling, such as splitting the data, fitting an appropriate model, testing / evaluating our model, and predicting against the test dataset. We will also later determine which model performs the best on our data.\n",
    "\n",
    "<span id=\"setup\"></span>\n",
    "\n",
    "## Setup\n",
    "\n",
    "**Question 1:** While it is possible to use categorical features in predictive modeling, for simplicity sake, we will filter out all non-numerical columns. Fill in the blanks such that only columns of type ```int``` or ```float``` remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "X_train = train.drop(labels=['totalyearlycompensation'], axis=1)\n",
    "y_train = train.loc[:, 'totalyearlycompensation']\n",
    "\n",
    "X_train = X_train.select_dtypes(...).drop(...)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encoding (OPTIONAL)\n",
    "\n",
    "Although we just removed all the categorical columns, those extra features can still be leveraged to potentially obtain a better model.\n",
    "\n",
    "Use sklearn's [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) object to transform the categorical columns to numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# train = pd.read_csv(\"train.csv\")\n",
    "# X_train = train.drop(labels=['totalyearlycompensation'], axis=1)\n",
    "# y_train = train.loc[:, 'totalyearlycompensation']\n",
    "\n",
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"validation\"></span>\n",
    "\n",
    "## Validation\n",
    "\n",
    "**Question 2:** Use the [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function to split up your training data into a training set and a validation set. The typical size of the validation set is also 20% of the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(..., ..., test_size=..., random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-Fold Cross Validation\n",
    "\n",
    "The validation method above is usable but not that robust. K-Fold Cross-Validation should be better.\n",
    "\n",
    "**Question 3:** Feel free to set up your own K-Fold cross-validation scheme. For more information, please read https://towardsdatascience.com/cross-validation-a-beginners-guide-5b8ca04962cd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this as an exercise, but you can continue rest of this notebook using single validation\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"rmse\"></span>\n",
    "\n",
    "## Accuracy & Error\n",
    "\n",
    "Our Kaggle competition uses Root-Mean-Square-Error (RMSE) as the error metric. In mathematical notation, it is:\n",
    "\n",
    "$$\\text{RMSE}(\\hat{y}, y) = \\sqrt{\\frac{1}{n} \\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}.$$\n",
    "\n",
    "**Question 4:** Complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse(y_true, y_pred):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"models\"></span>\n",
    "\n",
    "## Analyzing Different Models\n",
    "\n",
    "We will now analyze various different regressive models and compare how well they perform.\n",
    "\n",
    "### Linear Regression\n",
    "\n",
    "**Question 5:** Fit a linear regression model to your training data and report your RMSE.\n",
    "\n",
    "*Hint: Simply run the following cells*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate sklearn's linear regression object\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Fit linear regression model\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict against X_train using fitted model above\n",
    "lr_train_pred = lr.predict(X_train)\n",
    "\n",
    "# Calculate RMSE of predicted training output\n",
    "rmse(y_train, lr_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_val_pred = lr.predict(X_val)\n",
    "\n",
    "# Calculate RMSE of validation output\n",
    "## Notice our accuracy on the validation data is slightly worse\n",
    "rmse(y_val, lr_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "**Question 6:** Fit a random forest model to your data and report your RMSE.\n",
    "\n",
    "**NOTE:** If you're finding that your model is performing worse than your linear regression, make sure you tune the parameters to the RandomForestRegressor! Try to understand what the parameters mean by looking at the Decision Trees lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=...)\n",
    "rf.fit(..., ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_train_pred = rf.predict(...)\n",
    "rmse(y_train, rf_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_val_pred = rf.predict(...)\n",
    "rmse(y_val, rf_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like a case of overfitting since the training error >> validation error. Random forests / decision trees are notorious for overfitting as well due to how much they fit to the training data, but we can tune different parameters such as ```max_depth``` or ```n_estimators``` in order to increase bias and improve the model's predictive ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "Haha, don't get fooled here! Logistic regression (when combined with a decision rule) is really just a classification algorithm in disguise. In this case, we are looking for a numerical output, so logistic regression doesn't make sense at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression\n",
    "\n",
    "**Question 7:** Fit a ridge regression model and report your RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 8:** Do you notice a difference between the Ridge Regression model and the Linear Regression one? Does changing the regularization parameter increase or decrease accuracy? What does this tell you about the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Regression (OPTIONAL)\n",
    "\n",
    "Fit a support vector regression model and report your RMSE.\n",
    "\n",
    "**NOTE:** Support vectors machines (SVMs) often tend to overfit due to the nature of how they \"enforce\" correct classification of data points. This is also an out-of-scope topic, but it serves just to show that fancier models don't always prevail.\n",
    "\n",
    "If you would like to understand more about support vector regression, please read https://towardsdatascience.com/unlocking-the-true-power-of-support-vector-regression-847fd123a4a0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(C=0.001, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr.fit(X_train[:10000], y_train[:10000])    # Limiting to 10000 samples b/c SVR takes a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_train_pred = ...\n",
    "rmse(y_train, svr_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_val_pred = ...\n",
    "rmse(y_val, svr_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks (OPTIONAL)\n",
    "\n",
    "Train a neural network on the data. Report your RMSE.\n",
    "\n",
    "**NOTE**: Neural Networks require a lot of time to train and it is better to use GPU to train them. Kaggle provides free weekly GPU usage(37 hours/week). To use GPU, choose 'GPU' in the Accelerator from Settings located on the right side of your screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Own Model\n",
    "\n",
    "There's tons of regressive models out there to choose from. Some perform better in certain situations than others, and some require specific assumptions about the data to even work. If you would like to try out more models, please read https://towardsdatascience.com/7-of-the-most-commonly-used-regression-algorithms-and-how-to-choose-the-right-one-fc3c8890f9e3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id=\"submission\"></span>\n",
    "\n",
    "## Submission\n",
    "\n",
    "**Question 9:** Choose the model that performed best on the validation data. Use that model to predict against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_csv(\"test.csv\")\n",
    "X_test = X_test.select_dtypes(...).drop(...)\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = ...    # Choose best model to predict using\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cells to save your predicted test values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def results_to_csv(data, y_test):\n",
    "    y_test = y_test.astype(int)\n",
    "    df = pd.DataFrame({'totalyearlycompensation': y_test})\n",
    "    df.to_csv(data + '_submission_' + datetime.now().strftime(\"%Y_%m_%d-%H_%M_%S\") + '.csv',\n",
    "              index_label='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_csv(\"salary\", y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
