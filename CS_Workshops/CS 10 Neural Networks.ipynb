{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-3dc7e491-05f4-43c7-b22e-d6dcea8608d5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# [CS Series] Lecture 10: Neural Networks\n",
    "\n",
    "## May 8, 2022\n",
    "\n",
    "### Hosted by and maintained by the [Student Association for Applied Statistics (SAAS)](https://saas.berkeley.edu).\n",
    "Created by Ritvik Iyer and Akhil Vemuri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-2c0bb3ed-93f3-4aee-b80d-505ac99e5130",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Table of Contents\n",
    "1. [Gradient Descent](#grad_descent)\n",
    "2. [What are Neural Nets?](#what_are_nn)\n",
    "    1. [Neural net examples](#nn_examples)\n",
    "    2. [Neural network inspiration](#neuroscience)\n",
    "3. [The Single Neuron](#single_neuron)\n",
    "4. [Activation Functions](#activation_fn)\n",
    "5. [Linear Regression Connection](#lr-reg)\n",
    "6. [Perceptrons](#the-question)\n",
    "    1. [Single-layer Perceptrons](#slp)\n",
    "    2. [Multi-layer Perceptrons](#mlp)\n",
    "7. [Training a MLP on the Titanic Dataset](#titanic)  \n",
    "8. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00002-07fa0f83-70ca-46b0-b45b-d165f1e88958",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1647,
    "execution_start": 1619231537867,
    "source_hash": "5899e881",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.rcParams[\"figure.figsize\"] = [15.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-84b6c638-5b24-4f35-956f-a5f1ba14a09d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/AIsubsets.png\" alt=\"Drawing\" style=\"width: 500px; height: 380px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-57662683-d720-4174-92fa-cef50805cfbc",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/pikachu.jpeg\" alt=\"Drawing\" style=\"width: 300px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-8fd8c84d-82ef-481f-8eba-b3d71eb914ba",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='grad_descent'></a>\n",
    "## Gradient Descent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-59bbad4f-399f-4be6-b7b4-a67f0968085c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In many machine learning contexts, the objective is to minimize a *loss function*, which tells us the error in our predictions based on the parameters we chose. In linear regression, the loss function was mean squared error. Indeed, there many other types of loss functions such as cross entropy loss, zero-one loss, and more. Regardless of what loss function we choose, the objective is always to minimize it. Finding the minimum of a function can be exceptionally difficult, especially if the loss function is nonconvex. To make matters worse, we likely do not even know how the loss landscape looks with respect to the model's parameters. For instance, imagine the difference in difficulty in finding the minimum of a parabola and the minimum of a landscape shown below: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-41774732-7fbe-4df5-8269-0db9d14588b8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "![Picture title](neural_nets_images/image-20210418-121923.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-178888a4-1f6b-447d-8465-a554a1bf0eaa",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There may not even be a nice expression for the above function, so how do we even try to find the minimum? Enter gradient descent. The idea behind gradient descent is we take steps in the direction of steepest descent to land in a local minimum. Conveniently, the gradient of a function gives us the direction of steepest ascent, so we can just move in the opposite direction. More specifically:\n",
    "\n",
    "1) We calculate the gradient of the loss function with respect to the model parameters, $\\nabla_\\theta \\mathcal{L}(\\theta)$.\n",
    "\n",
    "2) We subtract some scalar multiple of our choice, $\\alpha$, of it from the current model parameters. This is like taking an $\\alpha$-sized step in the direction opposite the gradient.\n",
    "\n",
    "3) Repeat 1) and 2) until convergence.\n",
    "\n",
    "In math:\n",
    "\n",
    "1) Calculate $\\nabla_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "2) $\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta \\mathcal{L}(\\theta)$\n",
    "\n",
    "3) Repeat 1) and 2) until convergence.\n",
    "\n",
    "To demonstrate the power of gradient descent, we could use it to minimize linear regression loss, LASSO loss, ridge regression loss, and any convex function, without having to know how to solve it formulaically (although, it may take a bit longer). Essentially, you could use it as a black box solver for functions with no local minima, no saddle points, and no plateaus; and you could expect to find the gloabl optimum fairly easily. Indeed, there are several optimizers that are much better and more widely used, but they all draw heavily from the basic principles of gradient descent. Examples are stochastic gradient descent, AdaGrad, and Adam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00009-5fc5f12d-709e-4501-9b1a-363b66809888",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### GD Questions\n",
    "\n",
    "1) Does gradient descent guarantee that we will find the global optimum for every function?\n",
    "\n",
    "2) What happens when $\\alpha$ is large? What happens when $\\alpha$ is very close to $0$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00004-8ace803a-c3c7-495f-9659-edd005f74a26",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='what_are_nn'></a>\n",
    "\n",
    "## What are Neural Networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-df28ebe5-0c4f-443c-9d4a-451ee5cdab4d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Neural networks derive their name from the neural network we have in our head, our brain. In a very highly simplified model, the brain is a collection of neurons that receives electrical input signals from dendrites, outputting electrical signals via a single axon. Each neuron sends signals along a single axon and connects with other dendrites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-ca6f48b7-e3a2-4e40-84bd-ab67cb53dbd8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/neuron_connection.gif\" alt=\"Drawing\" style=\"width: 300px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-17e4025b-f85a-4606-a3cf-e1ce9d3afcdb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='nn_examples'></a>\n",
    "\n",
    "### Neural Networks -  Examples\n",
    "\n",
    "#### DALL-E - Creating Images from Text \n",
    "\n",
    "DALL-E is a neural network trained using image and text pairs in order to generate images from text descriptions. You can read more about it [here](https://openai.com/blog/dall-e/). Here is an example below of the images DALL-E created from the input text \"an armchair in the shape of an avocado.\"\n",
    "\n",
    "<img src=\"neural_nets_images/DallE_example.png\" alt=\"Drawing\" style=\"width: 800px; height: 600px\"/>\n",
    "\n",
    "<img src=\"neural_nets_images/dalle.png\" alt=\"Drawing\" style=\"width: 500px; height: 290px\"/>\n",
    "\n",
    "#### GPT-3 - Language Generation \n",
    "\n",
    "Generative Pre-trained Transformer 3 is an autoregressive language model that uses deep learning to produce human-like text. You can see some examples of how it is being used in real products [here](https://openai.com/blog/gpt-3-apps/). Below is an example of a conversation with a GPT-3 bot. \n",
    "\n",
    "<img src=\"neural_nets_images/gpt3exampl.png\" alt=\"Drawing\" style=\"width: 800px; height: 400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00006-7228c1d1-f24c-48d1-bc6c-48fd0fcc1d83",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='neuroscience'></a>\n",
    "\n",
    "### Neural Nets Were (Predictably) Inspired by Neuroscience\n",
    "\n",
    "Harvard neurophysiologists David H. Hubel and Torsten Wiesel recorded electrical brain activity from individual neurons in the brains of cats. They showed that some neurons are activated only when exposed to certain visual stimuli. When shown an animation of a rotating line, some neurons increased activity the more vertical the line got. They found that some neurons are specialized to detect certain visual cues, and that a combination of different neurons is necessary to comprehensively track a stimulus. \n",
    "\n",
    "This idea was adopted into the realm of neural nets, where multiple 'neurons' are strung together, often in a sequential order, to learn the label of their input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-68a031e9-8071-4c9a-ac4e-a32b088dc6dd",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/hubel_wiesel_cat.png\" alt=\"Drawing\" style=\"width: 400px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-16a1d260-8cfb-4fcd-ab48-569c2bdd00ab",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='single_neuron'></a>\n",
    "## The Single Neuron\n",
    "\n",
    "Every neural network is comprised of many neurons connected with weights. The neuron itself is the backbone of the network, and each neuron accomplishes the following tasks:\n",
    "\n",
    "\n",
    "1. Receives information through weights pointing to that neural multiplied by the neuron the weight originated from.\n",
    "2. Applies the activation function denoted by f to the sum of the weights multiplied by the previous neuron.\n",
    "3. Returns an result in the form of the outer layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-e13f3cce-7183-4dc8-b82a-bfbc918503d1",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/biological_neuron.jpeg\" alt=\"Drawing\" style=\"width: 400px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-e44f2221-d56c-409b-8f5a-6957a5067ea5",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The diagram above shows a single neuron taking in other neurons $x_0$, $x_1$ and $x_2$ with weights $w_0$, $w_1$ and $w_2$, respectively. The output from the neuron is the output axon. The function f is the Activation Function. The purpose of the activation function is to introduce non-linearity into the output of a neuron. This is necessary because using only linear functions to predict real world data is a poor decision. Greater accuracy requries neurons to learn nonlinear representations. We'll learn more about activation functions next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-4fda6742-fc7b-46de-b0b5-439aa06117bf",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='activation_fn'></a>\n",
    "\n",
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-fc847283-2b24-4e23-a271-cf64d3a187bc",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### What is an Activation Function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00011-f7fa7780-4809-4869-a985-5421dfd1d809",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "The purpose of an activation function is to help the network learn complex patterns in our data. An activation function introduces non-linear operations in the neural network. Without them, the network could only generate predictions from a series of (linear) matrix multiplications. \n",
    "\n",
    "<img src=\"neural_nets_images/ReLU.png\" alt=\"Drawing\" style=\"width: 500px; height: 300px\"/>\n",
    "\n",
    "An example of an activation function is the Rectified Linear Unit, or ReLU for short. As we see above, this is a non-linear function defined as: $f(x) = max(0, x)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-ea823010-87af-41ba-abfc-9d15ecf52bcb",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "$\\textit{Sigmoid}$ is useful if you want only positive numbers. However, it has fallen out of popularity recently because it causes gradients to vanish. When a neuron's activation saturates close to 0 or 1, the gradient will be really close to 0. During backpropogation this causes the signal to be lost. Also, because it is not 0-centered, it has a greater chance for gradient updates to go far in either direction.\n",
    "\n",
    "<img src=\"neural_nets_images/sigmoid.png\" alt=\"Graph\" style=\"width: 300px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-3135bd95-ba2a-4ab6-8548-6c564742e27d",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "$\\textit{Tanh}$ has many advantages over sigmoid since the activation function is centered at zero, and can output negative numbers. In practice, $\\textit{Tanh}$ is preferred over sigmoid, but it stil creates vanishing gradients problem when x becomes too large or too small.\n",
    "\n",
    "<img src=\"neural_nets_images/tanh.png\" alt=\"Graph\" style=\"width: 300px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lr-reg'></a>\n",
    "## Linear Regression Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is one of basic forms of machine learning we have learned. It simply fits the best linear model to a set of data points in order to determine a \"line of best fit\". But the simple linear regression model you've learned is actually a neural network in disguise!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div> <img src=\"https://joshuagoings.com/assets/linear.png\" width=\"500\"/> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of linear regression models as neural networks consisting of just a single artificial neuron, or as single-layer neural networks. Since for linear regression, every input is connected to every output (in this case there is only one output), we can regard this transformation as a *fully-connected layer*, or *dense layer*. We will talk more about single and multi-layer networks later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of neural network operations are linear as well, so there's no need to be alarmed by their seemingly complex nature. Each output node is simply a linear combination of weights and inputs, and there are potentially multiple output nodes. That's all there is to it! The only place where non-linearity is introduced is through the activation function, but this is only to ensure the ability to fit to non-linear trends in the data. The patterns therefore exhibited between OLS and neural nets are shockingly similar!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-5b5604f0-7b2a-40e0-976b-78e53e1bb12c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='perceptrons'></a>\n",
    "\n",
    "## Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-6249455a-9f5a-4cec-ba91-6ff660d2f937",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='slp'></a>\n",
    "\n",
    "### Single Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-4da52ea7-a989-4ad8-974a-9a250148fcc4",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "Remember the image of the neuron above? The combination of the input, the neuron, and the output was actually simplest example of a neural network, called a single-layer perceptron (SLP). An SLP is an algorithm for learning a binary classifier - that is, it classifies data into one of two classes based on some linear decision boundary.\n",
    "The architecture of the SLP consists of:\n",
    "\n",
    "1. **Input**: The SLP takes in multiple input values\n",
    "2. **Summation**: Each input value is multiplied by a weight (which is learned by the neural net), and summed together along with a bias term. \n",
    "3. **Activation function and output**: If the summation is less than some pre-defined threshold, we classify it as class 0. Otherwise, we label it as class 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00027-54e1c198-e0f9-45e9-a0c2-90491b85306a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/slp.png\" alt=\"Graph\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00031-3e8ef7cf-db3e-4100-881f-84c52e337ee7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "#### Training a SLP\n",
    "During training, we present each datapoint x and its label y to the SLP as inputs. We feed each training datapoint through the network and calculate the error in our prediction. Then, we use this error to update the weights and obtain a more accurate decision boundary. We iteratively repeat this process until the error is 0 and we obtain the best decision boundary for our training dataset.\n",
    "\n",
    "[Here](https://owenshen24.github.io/perceptron/) is an animated example of how a perceptron learns its decision boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-25ff3645-658d-47fb-8f0d-53bba0bbd171",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this example, the decision boundary learned by the perceptron shifts as more data is added.\n",
    "<img src=\"neural_nets_images/Perceptron_example.png\" alt=\"Graph\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-df9cc3cb-bf68-4453-a4a4-eac4d0aa8322",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "**Exercise**: What happens if we train an SLP on non-linearly separable data?\n",
    "\n",
    "Because the decision boundary is linear (since it takes the form $w_1x_1 + w_2x_2 + ... + w_nx_n + b$ = 0), an SLP can only classify linearly separable data. Our solution is to interconnect multiple perceptrons together to create a **multi-layer perceptron (MLP)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00033-eb33790c-b16a-4dbb-be71-dafb828ac735",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='mlp'></a>\n",
    "\n",
    "### Multi-layer Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00034-6ef85c8b-1978-40dc-8f51-397c1be8b5e8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<img src=\"neural_nets_images/mlp.png\" alt=\"mlp\" style=\"width: 400px; height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00035-f0922493-ed05-4656-9389-f079f5dfc53e",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "An MLP differs from an SLP because, as its name suggests, it has multiple layers of neurons. The architecture of the MLP consists of:\n",
    "\n",
    "1. **Input**: The MLP takes in multiple input values (same as SLP)\n",
    "2. **Multiple layers**: MLPs have multiple layers of neurons. MLPs are also **fully connected**, which means the output of each input neuron gets fed into the input of each neuron in the next layer. Each input value is multiplied by a weight (which is learned by the neural net), and summed together along with a bias term (same as SLP). \n",
    "3. **Activation function**: MLPs more often than not have nonlinear activation functions, like the sigmoid or ReLU functions above. This is because if we use linear activation functions in an MLP, it can easily be reduced down to a simple SLP. \n",
    "4. **Output layer**: The output of a MLP doesn't have to take the binary form an SLP does. MLPs can output one single value or a vector of values depending on the format of the problem. For example, an MLP for a multi-class classification problem can output multiple values, each corresponding to the probability of the input being a specific class.\n",
    "\n",
    "#### Training a Multi-layer Perceptron\n",
    "\n",
    "Training a MLP begins with a **forward pass**, in which the inputs are fed through the network. This is the same way inputs are passed into a SLP. After the forward pass is complete, we compare the output of the network to the expected output and calculate the error. This error is propogated back through the network until it reaches the first layer. This process is called **backpropogation** and is a vital step to learning for many machine learning networks. The weights are updated using gradient descent, which is based on the error of each neuron in the network, and the process repeats for a set period of time until the network has (hopefully) obtained better weights.\n",
    "\n",
    "**Exercise**: What is the difference in training a SLP vs MLP?\n",
    "\n",
    "MLPs can be used for a variety of problems, from simple binary classification tasks to complex tasks like image recognition and speech recognition. Let's use a MLP to solve a very familiar problem: predict whether or not someone survived on the Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-5bbd899d-b2c5-47be-8233-c999cdd392ff",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='titanic'></a>\n",
    "\n",
    "## Training a MLP on the Titanic Dataset 🤠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00040-58e203f1-f7c2-4c28-ad6f-f1de46c9795f",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "There are many cool framworks out there such as Pytorch, Tensorflow, and MXNet. The most popular ones are Pytorch and Tensorflow. Below we will use Pytorch, an open-source machine learning library developed by Facebook's AI Research lab. Don't worry about actually understanding the code- focus on understanding the comments instead!\n",
    "\n",
    "Our task today is to train a multi-layer perceptron to predict whether or not a particular passenger survived the Titanic accident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00045-0082b7ba-1d5d-4ec8-8c29-1e73b97f0c76",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 2353,
    "execution_start": 1619138792842,
    "source_hash": "a530cdbb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00046-69889b00-731e-45d1-96dc-b03896c4f859",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 17,
    "execution_start": 1619138795202,
    "source_hash": "d64c718a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#We read in our training and testing datapoints here.\n",
    "train = pd.read_csv(\"titanic/train.csv\")\n",
    "test = pd.read_csv(\"titanic/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00046-fd7d405f-47af-48fa-8684-fa64a1d7cb00",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1619138795233,
    "source_hash": "f7174307",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function preprocesses the data to feed as inputs to our model for training.\n",
    "def preprocess(df):\n",
    "    #make a deep copy so we don't change df\n",
    "    df1 = df.copy(deep=True)\n",
    "\n",
    "    df1.drop(['Name','Ticket','Cabin','PassengerId'],axis=1,inplace=True)\n",
    "    #one hot encoding\n",
    "    sex = pd.get_dummies(df1['Sex'],drop_first=True)\n",
    "    embark = pd.get_dummies(df1['Embarked'],drop_first=True)\n",
    "    pclass = pd.get_dummies(df1['Pclass'],drop_first=True)\n",
    "    df1 = pd.concat([df1,sex,embark,pclass],axis=1)\n",
    "\n",
    "    df1.drop(['Sex','Embarked', 'Pclass'],axis=1,inplace=True)\n",
    "\n",
    "    #to handle the test data, where we don't have the \"Survived\" column\n",
    "    if \"Survived\" in df1.columns:\n",
    "        y = df1.loc[:, 'Survived'].values\n",
    "        del df1['Survived']\n",
    "    else:\n",
    "        y = []\n",
    "\n",
    "    df1.fillna(df1.mean(),inplace=True)\n",
    "\n",
    "    # normalize input for NN to speed up learning and achieve faster convergence\n",
    "    Scaler1 = StandardScaler()\n",
    "\n",
    "    df1 = pd.DataFrame(Scaler1.fit_transform(df1))\n",
    "\n",
    "\n",
    "    X = df1.values\n",
    "\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00046-2250ed95-62cf-4f0b-8785-2e7d57e22681",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Declearing a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00046-70c96ed0-3719-414f-833b-ec1223e433e6",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1619138795244,
    "source_hash": "a985bce9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will declare a TitanicDataset class to make it easier to access the items we need.\n",
    "class TitanicDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "# This function basically does all the preprocessing of the data for us and returns it to us.\n",
    "    def __getitem__(self, idx):\n",
    "        X,y = preprocess(self.df)\n",
    "        X_idx = X[idx]\n",
    "        y_idx = y[idx]\n",
    "        return X_idx,y_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00048-f86533be-0d47-43bc-8b3d-453aa1f3aa73",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00047-189258c5-da2d-4bf4-bdab-63f9e5bcd3b5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 149,
    "execution_start": 1619233093810,
    "source_hash": "6965ea18",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This class represents our neural net architecture. \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        # Our neural net will have 3 fully connected layers. Notice how the first layer has an input of 9\n",
    "        #and an output of 512. This represents the dimensions how many input items are fed to the neural \n",
    "        #network (one for each feature column) and the number of inputs to our next fully connected layer. \n",
    "        self.fc1 = nn.Linear(9, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 2)\n",
    "\n",
    "        # We also include a dropout \"layer\", which will zero-out the elements of some neurons with \n",
    "        # probability 0.2 \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    # The forward function is the actual body of our neural net. \n",
    "    def forward(self, x):\n",
    "        # Our input gets fed into the first fully connected layer and the output goes into the next layer,\n",
    "        # after going through one round of dropout.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        # Our output gets fed into a sigmoid function for binary classification\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00050-68ba7680-bbb4-4e4a-8dbc-8646c63fb4cc",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00048-6079af25-14fd-4129-874f-6e67ff16d29f",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1619138795740,
    "source_hash": "fd30af43",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our loss function will be cross entropy loss.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# We will use stochastic gradient descent with a learning rate of 0.01 to find the optimum weights.\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00054-2aa00f87-0785-458c-a28f-2d4b3ee190b3",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00050-6e765674-7605-4ef4-8f6c-9f2587bfb2ba",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 3787,
    "execution_start": 1619138796857,
    "source_hash": "78bf7125",
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_epochs = 10\n",
    "batch_no = len(train) // batch_size\n",
    "\n",
    "train_loss = 0\n",
    "train_loss_min = np.Inf\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(batch_no):\n",
    "        # create index to generate different training batches\n",
    "        start = i*batch_size\n",
    "        end = start+batch_size\n",
    "\n",
    "        dfx, dfy = TitanicDataset(train)[start:end]\n",
    "        # convert arrays into tensors\n",
    "        x_var = Variable(torch.FloatTensor(dfx))\n",
    "        y_var = Variable(torch.LongTensor(dfy)) \n",
    "        \n",
    "        #clears old gradients from the last step\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_var)\n",
    "        loss = criterion(output,y_var)\n",
    "        #computes the derivative of the loss w.r.t. the parameters using backpropagation\n",
    "        loss.backward()\n",
    "        #making the optimizer to take a step based on the gradients of the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #torch.max finds the maximum value within each row of the output (we are looking at each row because dim=1)\n",
    "        #each row is contains 2 probability values that sum up to one (thanks to the sigmoid function in the model) \n",
    "        #we are essentially choosing the most probable output according to the probabilities\n",
    "        #values stands for the max probability value in each row of the output \n",
    "        #labels stands for the corresponding label(0 or 1) associated with the max probability value \n",
    "\n",
    "        values, labels = torch.max(output, 1)\n",
    "        #find the number of correct predictions\n",
    "        num_right = np.sum(labels.data.numpy() == dfy)\n",
    "        train_loss += loss.item()*batch_size\n",
    "    \n",
    "    train_loss = train_loss / len(train)\n",
    "    if train_loss <= train_loss_min:\n",
    "        print(\"Training loss decreased ({:6f} ===> {:6f}). Saving the model...\".format(train_loss_min,train_loss))\n",
    "        torch.save(model.state_dict(), \"model.pt\")\n",
    "        train_loss_min = train_loss\n",
    "    \n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        print('')\n",
    "        print(\"Epoch: {} \\tTrain Loss: {} \\tTrain Accuracy: {}\".format(epoch+1, train_loss,num_right / len(dfy) ))\n",
    "print('Training Ended! ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00053-d5a9644e-8c1f-42fd-b4b3-b7123dd4bc38",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00057-c832d6aa-3b8e-4138-93b5-9122725bed23",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 1,
    "execution_start": 1619138770648,
    "source_hash": "c566e482",
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test, _ = TitanicDataset(test)[:]\n",
    "X_test_var = Variable(torch.FloatTensor(X_test), requires_grad=False) \n",
    "#to perform inference without gradient calculation.\n",
    "with torch.no_grad():\n",
    "    test_result = model(X_test_var)\n",
    "values, labels = torch.max(test_result, 1)\n",
    "survived = labels.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00056-ae7f97df-b0cb-410f-8cc9-98c1faccd3d4",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "<a id='summary'></a>\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00055-4a95086f-b721-43a1-a4c3-a5646f3990a9",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "In this lesson, we introduced you to the machine learning topic of **neural nets**. We learned about how neural nets are implemented in the real world. Neural nets are inspired by the structure of the human brain. The building blocks of neural nets are neurons, which accepts inputs and passes them through an activation function. Activation functions like sigmoids and ReLUs are functions that help us learn complex patterns in the data. We also learned about single layer perceptrons, which is composed of an input layer, a neuron that multiplies the inputs by weights and performs aggregation, an activation function, and a binary output. However, they only make decisions on linearly-separable data. Multilayer perceptrons have no such limitation, since they are composed of multiiple SLPs and have non-linear activation functions. Finally, we ran through an example of a MLP being trained on the Titanic dataset to predict whether or not a particular passenger survived the accident."
   ]
  }
 ],
 "metadata": {
  "deepnote": {
   "is_reactive": false
  },
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "c8bc1fe3-33d9-473c-bd1c-a22e31080685",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
